[
{
	"uri": "//localhost:1313/",
	"title": "Automated Testing with AWS CodeBuild",
	"tags": [],
	"description": "",
	"content": "Workshop: Automated Testing with AWS CodeBuild and Parallel Execution Overview In this workshop, you\u0026rsquo;ll learn how to build a modern automated testing system for a web application built with .NET 8 MVC, leveraging AWS services such as CodeBuild, CloudWatch, and CodeQL.\nThrough hands-on sessions, you will:\nSet up a CI pipeline to trigger tests on every GitHub push Accelerate test execution with parallel execution strategies Integrate performance and security testing (via CodeQL) Monitor logs, analyze test results, and optimize cost Clean up AWS resources after testing Workshop Content Introduction \u0026amp; Objectives Environment Preparation Unit Test Automation Setup Parallel Execution \u0026amp; Result Aggregation Performance Testing Security Testing with CodeQL Monitoring, Reporting \u0026amp; Cost Optimization Cleanup \u0026amp; Resource Teardown "
},
{
	"uri": "//localhost:1313/7-monitoring-cost/7.1-cloudwatch-logs/",
	"title": "CloudWatch Logs",
	"tags": [],
	"description": "",
	"content": "Setting Up CloudWatch Logs Configure Log Groups [Insert screenshot: CloudWatch log groups setup]\nCreate Log Groups\naws logs create-log-group --log-group-name /aws/codebuild/test-automation aws logs put-retention-policy --log-group-name /aws/codebuild/test-automation --retention-in-days 30 [Insert screenshot: Log group creation]\nConfigure Log Streams [Insert screenshot: Log streams setup]\nBuild logs Test execution logs Performance metrics Set Up Metrics [Insert screenshot: Metrics configuration]\nCreate Metric Filters\naws logs put-metric-filter \\ --log-group-name /aws/codebuild/test-automation \\ --filter-name test-duration \\ --filter-pattern \u0026#34;[timestamp, duration]\u0026#34; \\ --metric-transformations \\ metricName=TestDuration,metricNamespace=TestAutomation,metricValue=$duration [Insert screenshot: Metric filter creation]\nConfigure Dashboards [Insert screenshot: Dashboard setup]\nCost metrics Resource usage Test execution stats Set Up Alerts [Insert screenshot: Alert configuration]\nCreate CloudWatch Alarms\nCost thresholds Resource limits Error rates [Insert screenshot: Alarm creation] Configure Notifications [Insert screenshot: Notification setup]\nEmail alerts SNS topics Integration webhooks Verification Checklist Log groups created Metrics configured Dashboards set up Alerts working Retention policies set Troubleshooting Guide [Insert screenshot: Common logging issues]\nLog Collection Issues\nMissing logs Delayed delivery Permission problems Metric Problems\nFilter matching Data aggregation Calculation errors Alert Issues\nNotification delivery Threshold settings Integration failures Best Practices [Insert screenshot: Logging best practices]\nLog Management\nStructured logging Clear categories Proper retention Metric Configuration\nRelevant metrics Proper aggregation Useful dimensions Next Steps After setting up CloudWatch logs, proceed to Analyze Cost\n"
},
{
	"uri": "//localhost:1313/6-security-testing/6.1-enable-codeql/",
	"title": "Enable CodeQL",
	"tags": [],
	"description": "",
	"content": "Enabling CodeQL Analysis Set Up CodeQL Environment [Insert screenshot: CodeQL setup]\nConfigure GitHub Repository\nEnable GitHub Actions Configure security settings Set up permissions [Insert screenshot: GitHub configuration] Create CodeQL Workflow\nname: \u0026#34;CodeQL Analysis\u0026#34; on: push: branches: [ main ] pull_request: branches: [ main ] schedule: - cron: \u0026#39;0 0 * * 0\u0026#39; jobs: analyze: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Initialize CodeQL uses: github/codeql-action/init@v2 with: languages: csharp [Insert screenshot: Workflow configuration]\nConfigure Analysis Settings [Insert screenshot: Analysis configuration]\nSet Up Query Suites\nSelect security queries Configure custom queries Set analysis scope [Insert screenshot: Query configuration] Configure Build Integration [Insert screenshot: Build integration]\nBuild settings Analysis triggers Result handling Set Up Notifications [Insert screenshot: Notification setup]\nConfigure Alerts Security alerts Workflow notifications Team notifications [Insert screenshot: Alert configuration] Verification Checklist CodeQL enabled Workflow configured Queries selected Notifications set Integration tested Troubleshooting Guide [Insert screenshot: Common CodeQL issues]\nSetup Issues\nPermission problems Configuration errors Integration failures Analysis Problems\nBuild failures Query errors Resource constraints Notification Issues\nAlert delivery Configuration problems Access rights Best Practices [Insert screenshot: CodeQL best practices]\nConfiguration\nRegular updates Clear documentation Proper scoping Analysis Management\nRegular reviews Performance monitoring Result tracking Next Steps After enabling CodeQL, proceed to Review Alerts\n"
},
{
	"uri": "//localhost:1313/1-introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Workshop Overview: Modern Test Automation with AWS CodeBuild \u0026amp; Parallel Execution Architecture Overview Prerequisites AWS Account with administrative access GitHub account Basic knowledge of .NET development Visual Studio Code or preferred IDE AWS CLI installed locally What You\u0026rsquo;ll Build In this workshop, you will:\nSet up a complete CI/CD pipeline for test automation Implement automated unit testing Configure parallel test execution Add performance testing Enable security scanning Monitor and optimize costs Time Estimation Total workshop time: ~8 hours\nEnvironment Setup: 1 hour Automated Testing: 2 hours Parallel Execution: 1.5 hours Performance Testing: 1.5 hours Security Testing: 1 hour Cost Monitoring: 0.5 hour Clean Up: 0.5 hour Cost Estimation Estimated AWS costs for completing this workshop:\nCodeBuild: ~$1-2 CloudWatch: ~$1 S3 Storage: \u0026lt; $1 Total: ~$3-4 Let\u0026rsquo;s begin with Environment Setup!\n"
},
{
	"uri": "//localhost:1313/4-parallel-execution/4.1-multiple-tests/",
	"title": "Multiple Tests Setup",
	"tags": [],
	"description": "",
	"content": "Setting Up Multiple Test Projects Create Test Structure [Insert screenshot: Solution structure in VS Code]\nCreate Test Projects\nUnit Tests Project Integration Tests Project Performance Tests Project [Insert screenshot: Project creation process] Configure Project References [Insert screenshot: Adding project references]\nAdd main project reference Configure build order Set up dependencies Organize Test Categories [Insert screenshot: Test organization structure]\nUnit Tests\nControllers Services Models [Insert screenshot: Unit test structure] Integration Tests [Insert screenshot: Integration test organization]\nAPI Tests Database Tests External Services Performance Tests [Insert screenshot: Performance test setup]\nLoad Tests Stress Tests Endurance Tests Configure Test Settings [Insert screenshot: Test configuration settings]\nTest Runner Settings\nExecution order Test categories Timeout settings [Insert screenshot: Runner configuration] Environment Configuration [Insert screenshot: Environment settings]\nTest environments Dependencies Resources Verification Checklist Projects created successfully References configured correctly Categories organized properly Settings applied correctly Build succeeding Troubleshooting Guide [Insert screenshot: Common setup issues]\nProject Structure\nReference issues Build order Namespace conflicts Configuration Problems\nSettings errors Environment setup Dependency issues Build Issues\nProject conflicts Version mismatches Path problems Best Practices [Insert screenshot: Project organization best practices]\nStructure Organization\nClear naming Logical grouping Consistent patterns Configuration Management\nEnvironment separation Shared settings Version control Next Steps After setting up multiple tests, proceed to Configure Parallel Execution\n"
},
{
	"uri": "//localhost:1313/2-environment-setup/2.1-website-setup/",
	"title": "Website Setup",
	"tags": [],
	"description": "",
	"content": "Website Setup Options You have two options for setting up the website for this workshop:\nOption A: Using the Sample Movie Website Fork the sample movie website repository:\nGo to https://github.com/Duyvh10102003/WebsiteXemPhim Click the \u0026ldquo;Fork\u0026rdquo; button in the top-right corner Select your GitHub account as the destination Clone your forked repository:\ngit clone https://github.com/[YOUR-USERNAME]/WebsiteXemPhim.git cd WebsiteXemPhim Switch to the HoangDuy branch:\ngit checkout HoangDuy Option B: Using Your Own Website If you prefer to use your own website, follow these steps:\nCreate a new GitHub repository:\nGo to https://github.com/new Name your repository Choose public or private visibility Initialize with a README if desired Push your website code:\ngit init git add . git commit -m \u0026#34;Initial commit\u0026#34; git branch -M main git remote add origin https://github.com/[YOUR-USERNAME]/[YOUR-REPO].git git push -u origin main Important Notes Ensure your website code is in the root directory of the repository The repository must be accessible to AWS CodeBuild If using a private repository, additional AWS CodeBuild configuration will be required Next Steps After completing either option, proceed to Required Tools Installation\n"
},
{
	"uri": "//localhost:1313/5-performance-testing/5.1-write-performance-tests/",
	"title": "Write Performance Tests",
	"tags": [],
	"description": "",
	"content": "Writing Performance Tests Set Up k6 Testing Tool [Insert screenshot: k6 installation]\nInstall k6\nDownload instructions Installation steps Verification process [Insert screenshot: Installation verification] Configure Environment [Insert screenshot: k6 configuration]\nBasic setup Environment variables Test structure Create Basic Load Test [Insert screenshot: Basic load test creation]\nWrite Load Test Script import http from \u0026#39;k6/http\u0026#39;; import { check, sleep } from \u0026#39;k6\u0026#39;; export const options = { stages: [ { duration: \u0026#39;1m\u0026#39;, target: 20 }, { duration: \u0026#39;2m\u0026#39;, target: 20 }, { duration: \u0026#39;1m\u0026#39;, target: 0 }, ], }; export default function () { const response = http.get(\u0026#39;http://test.api/endpoint\u0026#39;); check(response, { \u0026#39;is status 200\u0026#39;: (r) =\u0026gt; r.status === 200, }); sleep(1); } [Insert screenshot: Script execution] Implement Test Scenarios [Insert screenshot: Test scenario implementation]\nCreate Test Cases\nLoad testing Stress testing Spike testing [Insert screenshot: Test types] Configure Metrics [Insert screenshot: Metrics configuration]\nResponse times Error rates Throughput Verification Checklist k6 installed correctly Basic test running Metrics collecting Scenarios implemented Results recording Troubleshooting Guide [Insert screenshot: Common k6 issues]\nInstallation Issues\nPath problems Dependencies Version conflicts Script Problems\nSyntax errors Logic issues Resource limits Execution Issues\nConnection problems Resource constraints Timeout errors Best Practices [Insert screenshot: Performance test best practices]\nTest Design\nClear objectives Realistic scenarios Proper metrics Script Organization\nModular code Reusable functions Clear documentation Next Steps After writing performance tests, proceed to CI/CD Integration\n"
},
{
	"uri": "//localhost:1313/3-automated-unit-test/3.1-codebuild-setup/",
	"title": "Writing First Unit Test",
	"tags": [],
	"description": "",
	"content": "In this section, you will create a simple test file to ensure the CI/CD system can run unit tests correctly.\n‚úÖ Objectives Create a simple test file using xUnit Simulate test delays for performance testing Verify correctness and performance for future parallel execution üìÅ File Structure Create file at: Web.Tests/LuotXemTests.cs\nusing System.Threading; using Xunit; namespace Web.Tests { public class LuotXemTests { [Fact] public void Fake_LuotXem_Test1() { Thread.Sleep(1000); // simulate 1 second test duration Assert.True(true); } [Fact] public void Fake_LuotXem_Test2() { Thread.Sleep(1000); Assert.True(true); } } } üñº Implementation Steps Add your actual screenshot of the test file in Visual Studio here\n‚úÖ Verification Run the following command in the root directory to verify the tests:\ndotnet test Expected output:\nPassed! - Failed: 0, Passed: 2, Skipped: 0 Add your actual screenshot of the test results here\nüìå Notes The Thread.Sleep() calls are used to simulate test processing time. You\u0026rsquo;ll see significant improvements when applying Parallel Execution (section 4).\n"
},
{
	"uri": "//localhost:1313/7-monitoring-cost/7.2-analyze-cost/",
	"title": "Analyze Cost",
	"tags": [],
	"description": "",
	"content": "Analyzing Infrastructure Costs Set Up Cost Analysis [Insert screenshot: Cost analysis setup]\nConfigure Cost Explorer\nEnable Cost Explorer Set up cost categories Configure tags [Insert screenshot: Cost Explorer configuration] Create Cost Reports\n# cost_analysis.py def analyze_costs(start_date, end_date): costs = get_aws_costs(start_date, end_date) return { \u0026#39;total\u0026#39;: sum(costs), \u0026#39;by_service\u0026#39;: group_by_service(costs), \u0026#39;by_tag\u0026#39;: group_by_tag(costs), \u0026#39;trends\u0026#39;: analyze_trends(costs) } [Insert screenshot: Cost report generation]\nMonitor Resource Usage [Insert screenshot: Resource monitoring]\nTrack Resource Consumption\nCompute usage Storage usage Network traffic [Insert screenshot: Resource tracking] Analyze Usage Patterns [Insert screenshot: Usage analysis]\nPeak usage times Idle periods Resource efficiency Generate Cost Reports [Insert screenshot: Report generation]\nCreate Cost Dashboards\nService costs Resource costs Trend analysis [Insert screenshot: Dashboard creation] Set Up Regular Reports [Insert screenshot: Report scheduling]\nDaily summaries Weekly reports Monthly analysis Verification Checklist Cost Explorer enabled Reports configured Resources tracked Dashboards created Alerts set up Troubleshooting Guide [Insert screenshot: Common cost issues]\nData Collection Issues\nMissing data Delayed updates Integration problems Analysis Problems\nCalculation errors Tag issues Categorization problems Reporting Issues\nFormat problems Distribution failures Access issues Best Practices [Insert screenshot: Cost analysis best practices]\nCost Management\nRegular monitoring Clear categorization Proper tagging Resource Optimization\nUsage analysis Right-sizing Cost allocation Next Steps After analyzing costs, proceed to Optimize Config\n"
},
{
	"uri": "//localhost:1313/3-automated-unit-test/3.2-buildspec-setup/",
	"title": "Configure Buildspec for Unit Tests and Report Generation",
	"tags": [],
	"description": "",
	"content": "In this step, you will create a buildspec.yml file to:\nInstall test report generation tools Run unit tests with result logging Generate readable HTML test reports Prepare for result uploading and analysis ‚úÖ Objectives Install dotnet-reportgenerator-globaltool Run tests and export logs in .trx format Generate readable HTML reports üìÅ Directory Structure YourProject/ ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îî‚îÄ‚îÄ Web/ ‚îÇ ‚îî‚îÄ‚îÄ Web.Tests/ ‚îú‚îÄ‚îÄ buildspec.yml ‚îî‚îÄ‚îÄ TestReport/ ‚îî‚îÄ‚îÄ index.html üßæ Buildspec File Content version: 0.2 phases: install: runtime-versions: dotnet: 8.0 commands: - echo === Installing reportgenerator tool === - dotnet tool install -g dotnet-reportgenerator-globaltool - export PATH=\u0026#34;$PATH:/root/.dotnet/tools\u0026#34; build: commands: - echo === Restoring dependencies === - dotnet restore - echo === Building solution === - dotnet build --no-restore - echo === Running unit tests === - dotnet test Web.Tests/Web.Tests.csproj --logger \u0026#34;trx;LogFileName=test_results.trx\u0026#34; - echo === Generating HTML test report === - reportgenerator \u0026#34;-reports:**/test_results.trx\u0026#34; \u0026#34;-targetdir:TestReport\u0026#34; -reporttypes:Html artifacts: files: - TestReport/**/* discard-paths: no üí° Explanation Section Description dotnet tool install Installs HTML test report generator tool dotnet test + \u0026ndash;logger Writes test results to .trx file (Test Result XML) reportgenerator Generates HTML report from .trx file artifacts.files Specifies outputs to preserve (test reports) üîÑ Implementation Steps Create buildspec.yml in project root Copy buildspec content from guide Commit and push to repository Verify build in CodeBuild ‚úÖ Verification After pushing code, check AWS CodeBuild for:\nAutomatic build trigger Successful phase completion Artifacts containing HTML test report Open TestReport/index.html for detailed results Add your test report screenshot here\nüìå Notes .trx files contain detailed test result information HTML reports provide easy result viewing and analysis Artifacts are preserved for future reference This configuration forms the foundation for integration with other analysis tools "
},
{
	"uri": "//localhost:1313/4-parallel-execution/4.2-configure-parallel/",
	"title": "Configure Parallel Execution",
	"tags": [],
	"description": "",
	"content": "Configuring Parallel Test Execution Configure AWS CodeBuild [Insert screenshot: CodeBuild parallel configuration]\nUpdate Build Project\nEnable parallel builds Configure compute resources Set concurrency limits [Insert screenshot: Build settings] Configure Build Specification [Insert screenshot: BuildSpec configuration]\nversion: 0.2 batch: fast-fail: true build-graph: - identifier: UnitTests buildspec: unit.yml - identifier: IntegrationTests buildspec: integration.yml Set Up Test Parallelization [Insert screenshot: Test parallel settings]\nConfigure Test Runner\nSet max parallel tests Configure test groups Set execution order [Insert screenshot: Runner configuration] Resource Management [Insert screenshot: Resource settings]\nMemory allocation CPU utilization Network resources Environment Configuration [Insert screenshot: Environment setup]\nConfigure Test Environments\nSeparate environments Resource isolation Data separation [Insert screenshot: Environment isolation] Dependency Management [Insert screenshot: Dependency configuration]\nShared resources External services Test data Verification Checklist Parallel configuration complete Resources allocated properly Environments isolated Dependencies managed Build succeeding Troubleshooting Guide [Insert screenshot: Common parallel issues]\nResource Conflicts\nMemory issues CPU bottlenecks Network contention Environment Problems\nIsolation failures Resource sharing Configuration conflicts Build Issues\nConcurrency problems Timing issues Resource limits Best Practices [Insert screenshot: Parallel execution best practices]\nResource Management\nProper sizing Load balancing Resource monitoring Test Organization\nIndependent tests Grouped execution Clear dependencies Next Steps After configuring parallel execution, proceed to Aggregate Results\n"
},
{
	"uri": "//localhost:1313/2-environment-setup/",
	"title": "Environment Setup",
	"tags": [],
	"description": "",
	"content": "Overview This module guides you through setting up your development environment. You can either use our sample movie website or deploy your own website for testing.\nWhat You\u0026rsquo;ll Learn Website Setup Options\nOption A: Using the provided sample movie website Option B: Deploying your own website to GitHub Required Tools Installation\nAWS CLI installation and configuration Development tools setup Required SDK installations Prerequisites Before starting this module, ensure you have:\nAWS Account with appropriate permissions GitHub account Basic knowledge of Git and GitHub Administrative access to your development machine Time Estimation Total Module Time: ~30 minutes Individual Section Time: 10-15 minutes each Module Structure Website Setup\nUsing sample movie website OR deploying your own website Required Tools Installation\nDevelopment environment setup AWS tools installation Expected Outcomes By the end of this module, you will have:\nA working website repository on GitHub All necessary development tools installed AWS CLI configured and ready to use Let\u0026rsquo;s begin with Website Setup!\n"
},
{
	"uri": "//localhost:1313/5-performance-testing/5.2-integrate-into-ci/",
	"title": "Integrate into CI/CD",
	"tags": [],
	"description": "",
	"content": "Integrating Performance Tests into CI/CD Configure Build Pipeline [Insert screenshot: CodeBuild configuration]\nUpdate BuildSpec\nversion: 0.2 phases: install: commands: - curl -L https://github.com/grafana/k6/releases/download/v0.45.0/k6-v0.45.0-linux-amd64.tar.gz | tar xvz - mv k6-v0.45.0-linux-amd64/k6 /usr/local/bin build: commands: - k6 run performance-tests/load-test.js [Insert screenshot: BuildSpec configuration]\nSet Up Environment [Insert screenshot: Environment setup]\nConfigure resources Set variables Define limits Implement Test Automation [Insert screenshot: Test automation setup]\nCreate Test Workflow\nTrigger conditions Test sequence Result handling [Insert screenshot: Workflow configuration] Configure Resources [Insert screenshot: Resource configuration]\nCompute requirements Memory allocation Network settings Set Up Monitoring [Insert screenshot: Monitoring setup]\nConfigure CloudWatch\nMetrics collection Dashboard creation Alert configuration [Insert screenshot: CloudWatch settings] Set Up Notifications [Insert screenshot: Notification setup]\nAlert thresholds Notification channels Response actions Verification Checklist Pipeline configured Tests automated Resources allocated Monitoring active Notifications working Troubleshooting Guide [Insert screenshot: Common integration issues]\nPipeline Issues\nBuild failures Resource problems Configuration errors Test Execution\nTiming problems Resource constraints Network issues Monitoring Problems\nData collection Alert triggering Dashboard updates Best Practices [Insert screenshot: Integration best practices]\nPipeline Configuration\nClear stages Proper resources Error handling Test Management\nRegular execution Result tracking Performance monitoring Next Steps After integrating with CI/CD, proceed to Export Results\n"
},
{
	"uri": "//localhost:1313/2-environment-setup/2.2-install-tools/",
	"title": "Required Tools Installation",
	"tags": [],
	"description": "",
	"content": "Required Tools Installation To proceed with automated testing using AWS CodeBuild, you\u0026rsquo;ll need to install and configure several tools.\n1. AWS CLI Installation For Windows: Download the AWS CLI MSI installer:\nVisit: https://aws.amazon.com/cli/ Download the Windows x64 installer Run the installer:\nDouble-click the downloaded file Follow the installation wizard Verify installation by opening Command Prompt and running: aws --version For Linux: curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install For macOS: brew install awscli 2. Configure AWS CLI Get your AWS credentials:\nAccess Key ID Secret Access Key Default region (e.g., us-east-1) Run configuration:\naws configure Enter your credentials when prompted:\nAWS Access Key ID: [Your Access Key] AWS Secret Access Key: [Your Secret Key] Default region name: [Your Region] Default output format: json 3. Git Installation (if not already installed) For Windows: Download from: https://git-scm.com/download/win Run the installer with default options For Linux: sudo apt-get update sudo apt-get install git For macOS: brew install git 4. Visual Studio Code (Optional but Recommended) Download VS Code:\nVisit: https://code.visualstudio.com/ Choose your platform version Install recommended extensions:\nAWS Toolkit C# Dev Kit Git Extension Pack Verification Steps Verify AWS CLI: aws --version Verify Git: git --version Test AWS Configuration: aws sts get-caller-identity Troubleshooting AWS CLI Issues:\nEnsure your AWS credentials are correct Check if your IAM user has appropriate permissions Verify your default region is set correctly Git Issues:\nMake sure Git is in your system\u0026rsquo;s PATH Configure Git with your credentials: git config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your.email@example.com\u0026#34; Next Steps After installing and configuring all required tools, proceed to CodeBuild Project Setup\n"
},
{
	"uri": "//localhost:1313/6-security-testing/6.2-review-alerts/",
	"title": "Review Alerts",
	"tags": [],
	"description": "",
	"content": "Reviewing Security Alerts Access Security Alerts [Insert screenshot: Security alerts dashboard]\nNavigate to Security Tab\nOpen GitHub repository Go to Security tab View CodeQL alerts [Insert screenshot: Navigation path] Filter and Sort Alerts [Insert screenshot: Alert filtering]\nBy severity By status By type Analyze Alert Details [Insert screenshot: Alert analysis]\nReview Alert Information\nAlert description Affected code Potential impact [Insert screenshot: Alert details] Investigate Code Context [Insert screenshot: Code context]\nSource location Data flow Call hierarchy Prioritize Alerts [Insert screenshot: Alert prioritization]\nSet Priority Levels\nCritical issues High priority Medium priority Low priority [Insert screenshot: Priority settings] Create Action Plan [Insert screenshot: Action planning]\nImmediate fixes Scheduled repairs Long-term improvements Track Alert Status [Insert screenshot: Alert tracking]\nUpdate Alert Status Open In progress Resolved False positive [Insert screenshot: Status updates] Verification Checklist Alerts reviewed Priorities set Actions planned Status updated Team notified Troubleshooting Guide [Insert screenshot: Common alert issues]\nAccess Problems\nPermission issues Navigation errors Display problems Analysis Challenges\nComplex alerts False positives Missing context Tracking Issues\nStatus updates Progress monitoring Team coordination Best Practices [Insert screenshot: Alert review best practices]\nReview Process\nRegular schedule Team involvement Clear documentation Priority Management\nRisk assessment Resource allocation Timeline planning Next Steps After reviewing alerts, proceed to Fix Vulnerabilities\n"
},
{
	"uri": "//localhost:1313/4-parallel-execution/4.3-aggregate-results/",
	"title": "Aggregate Results",
	"tags": [],
	"description": "",
	"content": "Aggregating Parallel Test Results Configure Result Collection [Insert screenshot: Result collection setup]\nSet Up Results Directory\nCreate central location Configure permissions Set up structure [Insert screenshot: Directory structure] Configure Result Format [Insert screenshot: Result format settings]\nDefine output format Set up templates Configure metadata Implement Result Aggregation [Insert screenshot: Aggregation implementation]\nCreate Aggregation Script\n# Simple example of result aggregation def aggregate_results(result_files): total_results = { \u0026#39;passed\u0026#39;: 0, \u0026#39;failed\u0026#39;: 0, \u0026#39;duration\u0026#39;: 0 } for file in result_files: results = parse_results(file) update_totals(total_results, results) return total_results [Insert screenshot: Script execution]\nConfigure Report Generation [Insert screenshot: Report configuration]\nDefine report format Set up templates Configure distribution Set Up Results Dashboard [Insert screenshot: Dashboard setup]\nCreate Dashboard\nConfigure metrics Set up visualizations Define layouts [Insert screenshot: Dashboard layout] Configure Alerts [Insert screenshot: Alert configuration]\nSet thresholds Configure notifications Define actions Verification Checklist Result collection working Aggregation script running Reports generating Dashboard accessible Alerts configured Troubleshooting Guide [Insert screenshot: Common aggregation issues]\nCollection Issues\nMissing results Permission problems Path issues Aggregation Problems\nFormat errors Processing failures Memory issues Reporting Issues\nTemplate problems Generation errors Distribution failures Best Practices [Insert screenshot: Result management best practices]\nData Management\nRegular cleanup Proper archival Version control Report Organization\nClear structure Consistent format Easy navigation Next Steps After setting up result aggregation, proceed to Compare Speed\n"
},
{
	"uri": "//localhost:1313/3-automated-unit-test/",
	"title": "Automated Testing Framework",
	"tags": [],
	"description": "",
	"content": "In this module, we will build an automated testing framework for our movie streaming website using AWS CodeBuild and modern testing tools.\nüéØ Objectives Set up automated testing environment with AWS CodeBuild Write and run unit tests for components Generate detailed and readable test reports Automate testing process for new code changes üìã Main Content Writing First Unit Test\nCreate simple test file Basic test structure Simulate test delays Configure Buildspec\nCreate buildspec.yml file Install reporting tools Configure output artifacts Setup CodeBuild\nCreate CodeBuild project Connect with GitHub Configure build environment Verify Results\nRun and view build results Analyze test reports Handle errors (if any) ‚öôÔ∏è Prerequisites Basic understanding of .NET testing AWS CLI installed and configured GitHub account with repository ‚è± Estimated Time Total time: ~2 hours Each section: 25-30 minutes üìå Expected Outcomes After completing this module, you will have:\nComplete Testing Framework\nAutomated unit tests Detailed test reports GitHub integration CI/CD Pipeline\nAutomatic testing on code push Test results storage Error notifications Documentation \u0026amp; Reports\nHTML test reports Detailed build logs Test coverage statistics üõ† Tools Used AWS CodeBuild: Automated CI/CD service xUnit: .NET testing framework ReportGenerator: HTML report generation GitHub: Source code management üí° Pro Tips Read buildspec.yml configuration carefully Test locally before pushing Check logs for errors Organize tests with clear structure üîç Troubleshooting Check AWS access permissions Verify GitHub configuration Review detailed build logs Run tests locally for debugging Make sure to complete each step before moving to the next one\n"
},
{
	"uri": "//localhost:1313/5-performance-testing/5.3-export-results/",
	"title": "Export Results",
	"tags": [],
	"description": "",
	"content": "Exporting Performance Test Results Configure Result Export [Insert screenshot: Result export configuration]\nSet Up Export Format\n// k6 export configuration export const options = { ext: { loadimpact: { projectID: 123456, }, }, outputFields: { metrics: [\u0026#39;http_req_duration\u0026#39;, \u0026#39;http_reqs\u0026#39;, \u0026#39;vus\u0026#39;], trend: [\u0026#39;p95\u0026#39;, \u0026#39;avg\u0026#39;, \u0026#39;med\u0026#39;, \u0026#39;min\u0026#39;, \u0026#39;max\u0026#39;], }, }; [Insert screenshot: Export configuration]\nConfigure Storage Location [Insert screenshot: Storage setup]\nS3 bucket configuration File organization Retention policies Implement Data Processing [Insert screenshot: Data processing setup]\nCreate Processing Script\nParse test results Calculate metrics Generate summaries [Insert screenshot: Processing implementation] Set Up Data Pipeline [Insert screenshot: Data pipeline]\nData flow Transformation steps Output formats Create Reports [Insert screenshot: Report creation]\nConfigure Templates\nPerformance metrics Trend analysis Comparison views [Insert screenshot: Report templates] Set Up Distribution [Insert screenshot: Report distribution]\nEmail delivery Dashboard updates Notification system Verification Checklist Export configured Processing working Reports generating Distribution active Storage organized Troubleshooting Guide [Insert screenshot: Common export issues]\nExport Problems\nFormat errors Storage issues Permission problems Processing Issues\nData corruption Calculation errors Resource limits Report Problems\nTemplate errors Distribution failures Access issues Best Practices [Insert screenshot: Export best practices]\nData Management\nClear organization Regular cleanup Proper backup Report Design\nClear presentation Key metrics Actionable insights Next Steps After setting up result export, proceed to Analyze Performance\n"
},
{
	"uri": "//localhost:1313/6-security-testing/6.3-fix-vulnerabilities/",
	"title": "Fix Vulnerabilities",
	"tags": [],
	"description": "",
	"content": "Fixing Security Vulnerabilities Prepare Fix Environment [Insert screenshot: Fix environment setup]\nCreate Fix Branch\ngit checkout -b security-fix/issue-123 [Insert screenshot: Branch creation]\nSet Up Development Environment [Insert screenshot: Development setup]\nConfigure IDE Install security tools Set up testing environment Implement Security Fixes [Insert screenshot: Fix implementation]\nCommon Vulnerability Fixes\n// Before: SQL Injection vulnerability var query = $\u0026#34;SELECT * FROM Users WHERE Id = {userId}\u0026#34;; // After: Parameterized query var query = \u0026#34;SELECT * FROM Users WHERE Id = @UserId\u0026#34;; command.Parameters.AddWithValue(\u0026#34;@UserId\u0026#34;, userId); [Insert screenshot: Code changes]\nApply Security Best Practices [Insert screenshot: Security practices]\nInput validation Output encoding Secure configuration Error handling Test Security Fixes [Insert screenshot: Security testing]\nRun Security Tests\nUnit tests Integration tests Security scans [Insert screenshot: Test execution] Verify Fixes [Insert screenshot: Fix verification]\nCheck CodeQL results Run penetration tests Validate security controls Submit and Review Changes [Insert screenshot: Change submission]\nCreate Pull Request Detailed description Security impact Test results [Insert screenshot: PR creation] Verification Checklist Fix branch created Security fixes implemented Tests passing CodeQL clean PR submitted Troubleshooting Guide [Insert screenshot: Common fix issues]\nImplementation Issues\nCode conflicts Test failures Integration problems Security Concerns\nIncomplete fixes New vulnerabilities Side effects Review Problems\nDocumentation gaps Missing tests Unclear changes Best Practices [Insert screenshot: Security fix best practices]\nFix Implementation\nComplete solutions Thorough testing Clear documentation Code Review\nSecurity focus Comprehensive testing Future prevention Next Steps After fixing vulnerabilities, proceed to Configure Settings\n"
},
{
	"uri": "//localhost:1313/7-monitoring-cost/7.3-optimize-config/",
	"title": "Optimize Config",
	"tags": [],
	"description": "",
	"content": "Optimizing Infrastructure Configuration Analyze Current Configuration [Insert screenshot: Configuration analysis]\nReview Resource Settings\nInstance types Storage configurations Network settings [Insert screenshot: Resource settings] Identify Optimization Opportunities\n# optimization_analyzer.py def analyze_optimization(): return { \u0026#39;compute\u0026#39;: analyze_compute_usage(), \u0026#39;storage\u0026#39;: analyze_storage_usage(), \u0026#39;network\u0026#39;: analyze_network_usage(), \u0026#39;recommendations\u0026#39;: generate_recommendations() } [Insert screenshot: Optimization analysis]\nImplement Cost Optimizations [Insert screenshot: Optimization implementation]\nConfigure Resource Scaling\nAuto-scaling rules Schedule-based scaling Load-based adjustments [Insert screenshot: Scaling configuration] Optimize Storage Usage [Insert screenshot: Storage optimization]\nStorage class selection Lifecycle policies Retention rules Set Up Cost Controls [Insert screenshot: Cost control setup]\nConfigure Budgets\nService budgets Project budgets Alert thresholds [Insert screenshot: Budget configuration] Implement Cost Allocation [Insert screenshot: Cost allocation]\nTag-based allocation Service-based tracking Project attribution Verification Checklist Configuration reviewed Optimizations implemented Cost controls set Budgets configured Monitoring active Troubleshooting Guide [Insert screenshot: Common optimization issues]\nConfiguration Problems\nScaling issues Resource conflicts Permission errors Performance Impact\nService degradation Capacity issues Response times Cost Control Issues\nBudget overruns Allocation errors Tracking problems Best Practices [Insert screenshot: Optimization best practices]\nResource Management\nRegular reviews Performance monitoring Cost tracking Configuration Control\nChange management Documentation Version control Next Steps After optimizing configuration, proceed to Estimate Usage\n"
},
{
	"uri": "//localhost:1313/3-automated-unit-test/3.3-codebuild-project/",
	"title": "Setting up CodeBuild to Run Unit Tests",
	"tags": [],
	"description": "",
	"content": "In this section, you will create a CodeBuild project to automatically run unit tests from your GitHub source code, using the buildspec.yml file configured in the previous step.\nüéØ Objectives Create a project in AWS CodeBuild Connect GitHub with AWS (OAuth or PAT) Configure build environment Run buildspec.yml from repository (Optional) Store test reports in Amazon S3 üîß Implementation Steps 1Ô∏è‚É£ Access AWS CodeBuild Open: https://console.aws.amazon.com/codebuild/home Click Create build project 2Ô∏è‚É£ Enter Project Information Project name: ci-dotnet-unittest Description: Run unit tests and generate HTML report 3Ô∏è‚É£ Configure Source In the Source section, select Source provider as GitHub (Version 2) and select Manage account credentials if you haven\u0026rsquo;t connected GitHub yet. Click create a new GitHub connection to start creating connection A new window will open. Enter a name for the connection: Select Install a new app to connect to your GitHub Enter your GitHub password to connect After successful connection, you\u0026rsquo;ll see a confirmation message Return to the Manage default source credential page, select the connection you just created Back to CodeBuild page, select the connected repository and choose your project\u0026rsquo;s main branch (usually main or master) In Primary source webhook events section, ‚úÖ Check \u0026ldquo;Rebuild every time a code change is pushed to this repository\u0026rdquo; 4Ô∏è‚É£ Setup Build Environment Provisioning model: On-demand Environment image: Managed image Compute: EC2 Running mode: Container Operating System: Amazon Linux Runtime: Standard Image: aws/codebuild/amazonlinux-x86_64-standard:5.0 5Ô∏è‚É£ Buildspec file Buildspec: select Use a buildspec file Ensure your repo has buildspec.yml in the root directory 6Ô∏è‚É£ (Optional) Artifact for Reports If you want to store HTML test reports: Artifacts type: Amazon S3 S3 bucket: select previously created bucket 7Ô∏è‚É£ Create Project Click Create build project After creation: You can select Start build to test Or push code to GitHub for webhook to run automatically üí° Pro Tips If your repo lacks a buildspec.yml, you can use \u0026ldquo;Insert build commands\u0026rdquo;, but it\u0026rsquo;s not recommended After project creation, AWS will run tests automatically on \u0026ldquo;Start build\u0026rdquo; or push (if webhook enabled) With GitHub version 2, repos appear in a dropdown. PAT requires manual token entry "
},
{
	"uri": "//localhost:1313/2-environment-setup/2.3-create-s3/",
	"title": "2.3 - Create S3 Bucket",
	"tags": [],
	"description": "",
	"content": "ü™£ Create S3 Bucket In this section, you will create an S3 bucket to store test reports from AWS CodeBuild.\nüîß Implementation Steps 1Ô∏è‚É£ Access Amazon S3 Log in to AWS Console Find and select S3 service 2Ô∏è‚É£ Create Bucket Click Create bucket Enter bucket name: dotnet-unit-test-report Keep other settings as default Click Create bucket 3Ô∏è‚É£ Verify New bucket appears in the list Bucket status shows \u0026ldquo;Created\u0026rdquo; üìù Notes This bucket will be used to store test reports from CodeBuild Bucket name must be globally unique Default settings are sufficient for workshop purposes "
},
{
	"uri": "//localhost:1313/5-performance-testing/5.4-analyze-performance/",
	"title": "Analyze Performance",
	"tags": [],
	"description": "",
	"content": "Analyzing Performance Test Results Set Up Analysis Tools [Insert screenshot: Analysis tools setup]\nConfigure Analysis Environment\nInstall required tools Set up dependencies Configure workspace [Insert screenshot: Environment configuration] Import Test Data [Insert screenshot: Data import process]\nimport pandas as pd import numpy as np def load_test_data(file_path): data = pd.read_json(file_path) return process_test_data(data) Perform Data Analysis [Insert screenshot: Analysis process]\nCalculate Key Metrics\nResponse times Error rates Throughput [Insert screenshot: Metrics calculation] Generate Visualizations [Insert screenshot: Data visualization]\nPerformance trends Load patterns Error distribution Create Analysis Reports [Insert screenshot: Report creation]\nConfigure Report Templates\nPerformance summary Detailed analysis Recommendations [Insert screenshot: Report templates] Set Up Automated Analysis [Insert screenshot: Automation setup]\nScheduled analysis Alert thresholds Trend detection Verification Checklist Analysis tools working Data processing correct Visualizations clear Reports generating Automation running Troubleshooting Guide [Insert screenshot: Common analysis issues]\nData Problems\nMissing data Invalid formats Processing errors Analysis Issues\nCalculation errors Memory problems Performance bottlenecks Reporting Problems\nFormat issues Generation errors Distribution failures Best Practices [Insert screenshot: Analysis best practices]\nData Management\nRegular validation Clean processing Clear documentation Analysis Process\nStandard methods Consistent metrics Regular reviews Next Steps After analyzing performance, proceed to Security Testing\n"
},
{
	"uri": "//localhost:1313/4-parallel-execution/4.4-compare-speed/",
	"title": "Compare Speed",
	"tags": [],
	"description": "",
	"content": "Comparing Execution Speed Set Up Performance Measurement [Insert screenshot: Performance measurement setup]\nConfigure Metrics Collection\nExecution time Resource usage Test throughput [Insert screenshot: Metrics configuration] Create Baseline Measurements [Insert screenshot: Baseline testing]\nSequential execution Single thread performance Resource utilization Implement Comparison Tools [Insert screenshot: Comparison tool setup]\nCreate Comparison Script\n# Simple performance comparison def compare_execution_speeds(sequential_data, parallel_data): comparison = { \u0026#39;time_difference\u0026#39;: parallel_data[\u0026#39;duration\u0026#39;] - sequential_data[\u0026#39;duration\u0026#39;], \u0026#39;speedup_factor\u0026#39;: sequential_data[\u0026#39;duration\u0026#39;] / parallel_data[\u0026#39;duration\u0026#39;], \u0026#39;resource_efficiency\u0026#39;: calculate_efficiency(sequential_data, parallel_data) } return comparison [Insert screenshot: Script execution]\nGenerate Comparison Reports [Insert screenshot: Report generation]\nPerformance metrics Visual comparisons Trend analysis Create Performance Dashboard [Insert screenshot: Performance dashboard]\nConfigure Visualizations\nExecution times Resource usage Efficiency metrics [Insert screenshot: Dashboard layout] Set Up Monitoring [Insert screenshot: Monitoring setup]\nReal-time metrics Historical trends Alert thresholds Verification Checklist Metrics collection working Comparison tools running Reports generating Dashboard accessible Monitoring active Troubleshooting Guide [Insert screenshot: Common performance issues]\nMeasurement Issues\nTiming accuracy Data collection Metric reliability Comparison Problems\nData inconsistency Analysis errors Reporting issues Monitoring Challenges\nData gaps Alert accuracy Resource overhead Best Practices [Insert screenshot: Performance comparison best practices]\nData Collection\nConsistent methodology Regular sampling Clean data Analysis Process\nStandard metrics Clear comparisons Documented methodology Next Steps After comparing execution speeds, proceed to Performance Testing\n"
},
{
	"uri": "//localhost:1313/6-security-testing/6.4-disable-if-needed/",
	"title": "Configure Settings",
	"tags": [],
	"description": "",
	"content": "Configuring Security Settings Configure CodeQL Settings [Insert screenshot: CodeQL configuration]\nUpdate Analysis Configuration\nname: \u0026#34;CodeQL Config\u0026#34; queries: - uses: security-extended - uses: security-and-quality paths-ignore: - \u0026#39;**/test/**\u0026#39; - \u0026#39;**/generated/**\u0026#39; [Insert screenshot: Configuration file]\nSet Up Query Suites [Insert screenshot: Query suite setup]\nSelect query types Configure severity levels Set analysis scope Configure Alert Settings [Insert screenshot: Alert configuration]\nSet Alert Rules\nSeverity thresholds Notification rules Response actions [Insert screenshot: Alert rules] Configure Notifications [Insert screenshot: Notification setup]\nEmail notifications Integration alerts Team notifications Manage Security Controls [Insert screenshot: Security controls]\nAccess Controls\nRepository permissions Analysis access Report access [Insert screenshot: Access settings] Policy Configuration [Insert screenshot: Policy setup]\nSecurity policies Enforcement rules Compliance settings Verification Checklist CodeQL configured Alerts set up Notifications working Access controls verified Policies implemented Troubleshooting Guide [Insert screenshot: Common configuration issues]\nConfiguration Issues\nSyntax errors Permission problems Integration failures Alert Problems\nMissing notifications False positives Alert overload Access Issues\nPermission denials Authentication errors Integration problems Best Practices [Insert screenshot: Configuration best practices]\nSecurity Settings\nRegular reviews Clear documentation Team communication Alert Management\nPriority levels Response procedures Regular maintenance Next Steps After configuring security settings, proceed to Monitoring Cost\n"
},
{
	"uri": "//localhost:1313/7-monitoring-cost/7.4-estimate-usage/",
	"title": "Estimate Usage",
	"tags": [],
	"description": "",
	"content": "Estimating Resource Usage Analyze Historical Usage [Insert screenshot: Usage analysis]\nCollect Usage Data\n# usage_analyzer.py def analyze_historical_usage(): return { \u0026#39;compute\u0026#39;: get_compute_metrics(), \u0026#39;storage\u0026#39;: get_storage_metrics(), \u0026#39;network\u0026#39;: get_network_metrics(), \u0026#39;patterns\u0026#39;: identify_patterns() } [Insert screenshot: Data collection]\nIdentify Usage Patterns [Insert screenshot: Pattern analysis]\nPeak usage times Seasonal variations Growth trends Create Usage Forecasts [Insert screenshot: Forecast creation]\nGenerate Predictions\nShort-term forecasts Medium-term projections Long-term estimates [Insert screenshot: Prediction models] Calculate Resource Requirements [Insert screenshot: Resource calculation]\nCompute needs Storage requirements Network capacity Plan Resource Allocation [Insert screenshot: Resource planning]\nCreate Capacity Plans\nResource scaling Growth accommodation Buffer allocation [Insert screenshot: Capacity planning] Set Up Resource Tracking [Insert screenshot: Resource tracking]\nUsage monitoring Threshold alerts Trend analysis Verification Checklist Historical data analyzed Patterns identified Forecasts generated Resources planned Monitoring configured Troubleshooting Guide [Insert screenshot: Common estimation issues]\nData Problems\nIncomplete data Inconsistent metrics Analysis errors Forecast Issues\nAccuracy problems Pattern recognition Seasonal adjustments Planning Challenges\nResource allocation Capacity planning Growth estimation Best Practices [Insert screenshot: Estimation best practices]\nData Management\nRegular collection Data validation Pattern analysis Forecast Accuracy\nMultiple models Regular updates Validation checks Next Steps After estimating usage, proceed to Clean Up\n"
},
{
	"uri": "//localhost:1313/4-parallel-execution/",
	"title": "Parallel Test Execution",
	"tags": [],
	"description": "",
	"content": "Th·ª±c thi test song song v·ªõi AWS CodeBuild Trong module n√†y, ch√∫ng ta s·∫Ω c·∫•u h√¨nh th·ª±c thi song song c√°c test case ƒë·ªÉ gi·∫£m th·ªùi gian ch·∫°y test t·ªïng th·ªÉ.\nC·∫•u h√¨nh AWS CodeBuild T·∫°o file buildspec.yml: version: 0.2 phases: install: runtime-versions: nodejs: 18 commands: - npm install build: commands: - echo \u0026#34;B·∫Øt ƒë·∫ßu ch·∫°y test song song\u0026#34; - | npm run test:components \u0026amp; \\ npm run test:api \u0026amp; \\ npm run test:e2e \u0026amp; \\ wait post_build: commands: - node scripts/merge-test-results.js reports: test-reports: files: - \u0026#39;reports/**/*\u0026#39; file-format: JunitXml artifacts: files: - reports/**/* - coverage/**/* C·∫•u h√¨nh Script trong package.json { \u0026#34;scripts\u0026#34;: { \u0026#34;test:components\u0026#34;: \u0026#34;jest --config jest.components.config.js --maxWorkers=2\u0026#34;, \u0026#34;test:api\u0026#34;: \u0026#34;jest --config jest.api.config.js --maxWorkers=2\u0026#34;, \u0026#34;test:e2e\u0026#34;: \u0026#34;cypress run --parallel --record --key your-key\u0026#34;, \u0026#34;test:all\u0026#34;: \u0026#34;npm-run-all --parallel test:*\u0026#34; } } Script g·ªôp k·∫øt qu·∫£ test // scripts/merge-test-results.js const fs = require(\u0026#39;fs\u0026#39;); async function mergeResults() { const results = { components: require(\u0026#39;../reports/component-results.json\u0026#39;), api: require(\u0026#39;../reports/api-results.json\u0026#39;), e2e: require(\u0026#39;../reports/e2e-results.json\u0026#39;) }; const summary = { totalTests: 0, passed: 0, failed: 0, duration: 0 }; Object.values(results).forEach(result =\u0026gt; { summary.totalTests += result.totalTests; summary.passed += result.passedTests; summary.failed += result.failedTests; summary.duration += result.duration; }); fs.writeFileSync(\u0026#39;reports/summary.json\u0026#39;, JSON.stringify(summary, null, 2)); } mergeResults(); L·ª£i √≠ch c·ªßa th·ª±c thi song song Gi·∫£m th·ªùi gian ch·∫°y test\nCh·∫°y nhi·ªÅu test c√πng l√∫c T·∫≠n d·ª•ng t·ªëi ƒëa t√†i nguy√™n T·ªëi ∆∞u t√†i nguy√™n\nS·ª≠ d·ª•ng hi·ªáu qu·∫£ AWS CodeBuild Ti·∫øt ki·ªám chi ph√≠ C√°c b∆∞·ªõc th·ª±c hi·ªán T·ªï ch·ª©c test theo lo·∫°i: tests/ ‚îú‚îÄ‚îÄ components/ ‚îÇ ‚îú‚îÄ‚îÄ MovieCard.test.js ‚îÇ ‚îî‚îÄ‚îÄ MovieList.test.js ‚îú‚îÄ‚îÄ api/ ‚îÇ ‚îú‚îÄ‚îÄ movieService.test.js ‚îÇ ‚îî‚îÄ‚îÄ authService.test.js ‚îî‚îÄ‚îÄ e2e/ ‚îú‚îÄ‚îÄ browse.spec.js ‚îî‚îÄ‚îÄ watch.spec.js C·∫•u h√¨nh Jest cho t·ª´ng lo·∫°i test: // jest.components.config.js module.exports = { testMatch: [\u0026#39;\u0026lt;rootDir\u0026gt;/tests/components/**/*.test.js\u0026#39;], // ... c√°c c·∫•u h√¨nh kh√°c }; // jest.api.config.js module.exports = { testMatch: [\u0026#39;\u0026lt;rootDir\u0026gt;/tests/api/**/*.test.js\u0026#39;], // ... c√°c c·∫•u h√¨nh kh√°c }; C·∫•u h√¨nh Cypress cho E2E test: // cypress.config.js module.exports = { e2e: { setupNodeEvents(on, config) {}, specPattern: \u0026#39;tests/e2e/**/*.spec.js\u0026#39;, supportFile: \u0026#39;tests/e2e/support/index.js\u0026#39; } }; Gi√°m s√°t v√† b√°o c√°o T√≠ch h·ª£p v·ªõi CloudWatch: # buildspec.yml b·ªï sung env: variables: CLOUDWATCH_NAMESPACE: \u0026#34;MovieApp/Tests\u0026#34; phases: post_build: commands: - aws cloudwatch put-metric-data --namespace ${CLOUDWATCH_NAMESPACE} --metric-name TestDuration --value $DURATION - aws cloudwatch put-metric-data --namespace ${CLOUDWATCH_NAMESPACE} --metric-name TestsPassed --value $PASSED_TESTS T·∫°o Dashboard theo d√µi: Th·ªùi gian ch·∫°y test T·ª∑ l·ªá test pass/fail S·ªë l∆∞·ª£ng test th·ª±c thi B∆∞·ªõc ti·∫øp theo Chuy·ªÉn sang ph·∫ßn Performance Testing C·∫•u h√¨nh th√¥ng b√°o k·∫øt qu·∫£ test Thi·∫øt l·∫≠p gi√°m s√°t li√™n t·ª•c "
},
{
	"uri": "//localhost:1313/3-automated-unit-test/3.4-verify-results/",
	"title": "Verify Automatic Test Results on Code Push",
	"tags": [],
	"description": "",
	"content": "In this section, you will push code to GitHub and observe automatic test results from AWS CodeBuild.\nüéØ Objectives Push code to GitHub repository Watch CodeBuild run tests automatically Check test results and reports üîß Implementation Steps 1Ô∏è‚É£ Push Code to GitHub git add . git commit -m \u0026#34;Test trigger CodeBuild from GitHub\u0026#34; git push origin main 2Ô∏è‚É£ Monitor Automatic Build Go to AWS CodeBuild console Select project ci-dotnet-unittest Observe new build being automatically created 3Ô∏è‚É£ View Build Details In the Build details tab, you can monitor:\nPhase details: Steps being executed Build logs: Detailed test process logs Environment variables: Environment variables in use 4Ô∏è‚É£ Check Test Results in CloudWatch Go to CloudWatch Logs Find CodeBuild Log group View detailed logs of the recent build In the logs, look for:\nRunning tests... Passed! - Failed: 0, Passed: 2, Skipped: 0 Test Run Successful. 5Ô∏è‚É£ View Test Report in S3 Go to configured S3 bucket Find TestReport directory Download and open index.html üí° Pro Tips Monitor build status:\nBookmark build history page Enable email notifications Install AWS CLI for status checks Effective debugging:\n# View latest build logs aws codebuild batch-get-builds --ids \u0026lt;build-id\u0026gt; Time optimization:\nPush meaningful code Run tests locally first Verify buildspec syntax üìù Notes Automatic builds only run on configured branch pushes Each push creates a new build Logs are stored in CloudWatch HTML reports update after each build Always check build status before leaving your computer\n"
},
{
	"uri": "//localhost:1313/5-performance-testing/",
	"title": "Performance Testing",
	"tags": [],
	"description": "",
	"content": "Performance Testing Overview This module focuses on implementing performance testing in your CI/CD pipeline using AWS CodeBuild. You\u0026rsquo;ll learn how to write, execute, and analyze performance tests to ensure your application meets performance requirements.\nWhat You\u0026rsquo;ll Learn Writing Performance Tests\nTest scenarios design Load test implementation Stress test creation Performance metrics CI/CD Integration\nPipeline configuration Test automation Resource management Environment setup Results Export\nData collection Metrics aggregation Report generation Trend analysis Performance Analysis\nMetrics interpretation Bottleneck identification Optimization strategies Recommendations Prerequisites Before starting this module, ensure you have:\nCompleted Module 4 (Parallel Execution) Understanding of performance concepts Familiarity with k6 testing tool Basic knowledge of metrics analysis Time Estimation Total Module Time: ~3 hours Individual Section Time: 45 minutes each Module Structure Write Performance Tests\nTest scenario creation Performance metrics setup Integrate into CI/CD\nPipeline integration Automation setup Export Results\nData collection Report generation Analyze Performance\nAnalysis techniques Optimization strategies Expected Outcomes By the end of this module, you will have:\nCreated comprehensive performance tests Integrated tests into CI/CD pipeline Implemented results collection Analyzed performance metrics Identified optimization opportunities Let\u0026rsquo;s begin with Writing Performance Tests!\n"
},
{
	"uri": "//localhost:1313/6-security-testing/",
	"title": "Security Testing",
	"tags": [],
	"description": "",
	"content": "Security Testing Overview This module focuses on implementing security testing in your CI/CD pipeline using AWS CodeBuild and CodeQL. You\u0026rsquo;ll learn how to identify security vulnerabilities, analyze code for potential security issues, and implement security best practices.\nWhat You\u0026rsquo;ll Learn Enable CodeQL Analysis\nSetup configuration Language support Query selection Integration setup Review Security Alerts\nAlert analysis Severity levels False positive handling Prioritization Fix Security Issues\nVulnerability remediation Code improvements Security patterns Best practices Configure Security Settings\nAlert configuration Scan scheduling Access control Reporting setup Prerequisites Before starting this module, ensure you have:\nCompleted Module 5 (Performance Testing) Understanding of security concepts GitHub repository access AWS CodeBuild configuration Time Estimation Total Module Time: ~2.5 hours Individual Section Time: 35-40 minutes each Module Structure Enable CodeQL\nSetup configuration Integration setup Review Alerts\nAlert analysis Issue prioritization Fix Vulnerabilities\nIssue remediation Security improvements Configure Settings\nSecurity configuration Scan management Expected Outcomes By the end of this module, you will have:\nConfigured CodeQL analysis Implemented security scanning Reviewed security alerts Fixed security vulnerabilities Managed security settings Let\u0026rsquo;s begin with Enable CodeQL!\n"
},
{
	"uri": "//localhost:1313/7-monitoring-cost/",
	"title": "Monitoring Cost",
	"tags": [],
	"description": "",
	"content": "Monitoring Cost Overview This module focuses on monitoring and optimizing the costs associated with your automated testing infrastructure on AWS. You\u0026rsquo;ll learn how to track resource usage, analyze costs, and implement cost optimization strategies.\nWhat You\u0026rsquo;ll Learn CloudWatch Logs\nLog configuration Log analysis Metric extraction Alert setup Cost Analysis\nResource tracking Usage patterns Cost breakdown Trend analysis Configuration Optimization\nResource sizing Scheduling Automation Cost reduction Usage Estimation\nCapacity planning Growth projection Budget planning Resource forecasting Prerequisites Before starting this module, ensure you have:\nCompleted Module 6 (Security Testing) Understanding of AWS billing Access to CloudWatch Basic knowledge of cost management Time Estimation Total Module Time: ~2 hours Individual Section Time: 30 minutes each Module Structure CloudWatch Logs\nLog configuration Metric creation Analyze Cost\nCost analysis Usage patterns Optimize Config\nResource optimization Cost reduction Estimate Usage\nUsage projection Capacity planning Expected Outcomes By the end of this module, you will have:\nConfigured CloudWatch monitoring Analyzed resource costs Optimized configurations Estimated future usage Implemented cost controls Let\u0026rsquo;s begin with CloudWatch Logs!\n"
},
{
	"uri": "//localhost:1313/8-clean-up/",
	"title": "Clean Up Resources",
	"tags": [],
	"description": "",
	"content": "Cleaning Up Workshop Resources Prepare for Cleanup [Insert screenshot: Cleanup preparation]\nCreate Resource Inventory\n# List AWS resources aws resourcegroupstaggingapi get-resources \\ --tag-filters Key=Project,Values=test-automation [Insert screenshot: Resource listing]\nVerify Resource Dependencies [Insert screenshot: Dependency verification]\nService dependencies Resource relationships Data dependencies Remove AWS Resources [Insert screenshot: Resource removal]\nDelete CodeBuild Resources\n# Delete CodeBuild project aws codebuild delete-project --name test-automation # Delete artifacts bucket aws s3 rm s3://test-artifacts-bucket --recursive aws s3 rb s3://test-artifacts-bucket [Insert screenshot: CodeBuild cleanup]\nClean Up CloudWatch Resources\n# Delete log groups aws logs delete-log-group --log-group-name /aws/codebuild/test-automation # Delete dashboards aws cloudwatch delete-dashboards --dashboard-names test-automation [Insert screenshot: CloudWatch cleanup]\nVerify Cleanup [Insert screenshot: Cleanup verification]\nCheck Resource Status\nVerify deletions Check remaining resources Confirm cleanup [Insert screenshot: Status checking] Review Billing Impact [Insert screenshot: Billing review]\nCheck current charges Verify resource termination Monitor final bills Verification Checklist Resources identified Dependencies resolved Resources deleted Cleanup verified Billing checked Troubleshooting Guide [Insert screenshot: Common cleanup issues]\nDeletion Problems\nResource locks Permission issues Dependency conflicts Verification Issues\nHidden resources Delayed deletions Billing updates Dependency Problems\nService connections Resource links Data relationships Best Practices [Insert screenshot: Cleanup best practices]\nResource Management\nSystematic removal Dependency handling Verification steps Documentation\nResource tracking Cleanup procedures Issue resolution Final Steps After cleanup, verify all resources are removed and no unexpected charges occur.\n"
},
{
	"uri": "//localhost:1313/8-clean-up/cleanup/",
	"title": "Cleanup Instructions",
	"tags": [],
	"description": "",
	"content": "Resource Cleanup Instructions Overview Follow these steps carefully to remove all resources created during the workshop. The steps are ordered to handle dependencies correctly and prevent any orphaned resources.\nCleanup Steps 1. CodeBuild Resources Delete CodeBuild projects: # List all CodeBuild projects aws codebuild list-projects # Delete each project aws codebuild delete-project --name dotnet-test-automation aws codebuild delete-project --name performance-test-project Remove build artifacts: # Delete S3 bucket contents aws s3 rm s3://test-results-bucket --recursive # Delete the bucket aws s3 delete-bucket --bucket test-results-bucket 2. CloudWatch Resources Delete log groups: # Delete CodeBuild log groups aws logs delete-log-group --log-group-name /aws/codebuild/test-automation # Delete test execution log groups aws logs delete-log-group --log-group-name /aws/test-execution Remove CloudWatch dashboards: # Delete custom dashboards aws cloudwatch delete-dashboards --dashboard-names TestAutomationDashboard Delete alarms: # Delete cost and performance alarms aws cloudwatch delete-alarms --alarm-names HighTestingCost HighResourceUtilization 3. IAM Resources Remove IAM roles: # First detach policies aws iam detach-role-policy \\ --role-name CodeBuildServiceRole \\ --policy-arn arn:aws:iam::aws:policy/AWSCodeBuildAdminAccess # Then delete role aws iam delete-role --role-name CodeBuildServiceRole Delete custom policies: # List and delete custom policies aws iam list-policies --scope Local aws iam delete-policy --policy-arn \u0026lt;policy-arn\u0026gt; 4. GitHub Integration Remove GitHub connection:\nGo to AWS CodeBuild console Navigate to Source providers Delete GitHub connection Delete webhooks:\nGo to GitHub repository settings Navigate to Webhooks Delete AWS CodeBuild webhook 5. Verification Steps Verify resource deletion: # Check CodeBuild projects aws codebuild list-projects # Check CloudWatch log groups aws logs describe-log-groups # Check IAM roles aws iam list-roles | grep CodeBuild Check AWS Cost Explorer: Verify no ongoing charges Check for any remaining resources Monitor billing for next few days Best Practices Resource Tracking\nMaintain resource inventory Document dependencies Use tags effectively Regular audits Cleanup Process\nFollow dependency order Verify each step Document issues Backup important data Cost Management\nMonitor billing Set up alerts Regular checks Document expenses Common Issues and Solutions Dependency Conflicts\nFollow correct order Force delete if needed Check dependencies Document errors Permission Issues\nVerify IAM roles Check permissions Use admin account Document access Resource Lock\nCheck resource state Wait for completion Force termination Document locks Final Verification Run verification script: #!/bin/bash echo \u0026#34;Checking CodeBuild resources...\u0026#34; aws codebuild list-projects echo \u0026#34;Checking CloudWatch resources...\u0026#34; aws logs describe-log-groups aws cloudwatch describe-alarms echo \u0026#34;Checking IAM resources...\u0026#34; aws iam list-roles | grep CodeBuild echo \u0026#34;Checking S3 buckets...\u0026#34; aws s3 ls | grep test-results if [ $? -eq 0 ]; then echo \u0026#34;Warning: Some resources might still exist\u0026#34; else echo \u0026#34;All resources successfully cleaned up\u0026#34; fi Document cleanup status: # Cleanup Status Report Date: $(date) Status: Complete/Incomplete ## Resources Checked - CodeBuild projects: [Status] - CloudWatch resources: [Status] - IAM roles: [Status] - S3 buckets: [Status] ## Issues Encountered - [List any issues] ## Follow-up Actions - [List any required actions] Next Steps Monitor AWS billing for the next billing cycle Remove any workshop-related local files Document lessons learned Update team documentation "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]