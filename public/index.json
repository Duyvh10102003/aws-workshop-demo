[
{
	"uri": "//localhost:1313/",
	"title": "Automated Testing with AWS CodeBuild",
	"tags": [],
	"description": "",
	"content": "Workshop: Automated Testing with AWS CodeBuild and Parallel Execution Overview In this workshop, you\u0026rsquo;ll learn how to build a modern automated testing system for a web application built with .NET 8 MVC, leveraging AWS services such as CodeBuild, CloudWatch, and CodeQL.\nThrough hands-on sessions, you will:\nSet up a CI pipeline to trigger tests on every GitHub push Accelerate test execution with parallel execution strategies Integrate performance and security testing (via CodeQL) Monitor logs, analyze test results, and optimize cost Clean up AWS resources after testing Workshop Content Introduction \u0026amp; Objectives Environment Preparation Unit Test Automation Setup Parallel Execution \u0026amp; Result Aggregation Performance Testing Security Testing with CodeQL Monitoring, Reporting \u0026amp; Cost Optimization Cleanup \u0026amp; Resource Teardown "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": ["AWS", "DevOps", "CI/CD", "Automated Testing", ".NET 8"],
	"description": "Build an advanced automated testing pipeline for .NET 8 MVC apps using AWS CodeBuild and CodeQL",
	"content": "üß™ Workshop: Ki·ªÉm th·ª≠ t·ª± ƒë·ªông v·ªõi AWS CodeBuild v√† th·ª±c thi song song Automated Testing with AWS CodeBuild \u0026amp; Parallel Execution Introduction In the modern software development lifecycle, product quality and time-to-market are mission-critical. That\u0026rsquo;s why automated testing and cloud-based CI/CD pipelines have become essential industry standards.\nüéØ Workshop Objectives This workshop guides you through implementing a complete automated testing pipeline for a web application built with .NET 8 MVC, featuring:\n‚úÖ Auto-triggered unit tests upon GitHub commits ‚öôÔ∏è Parallel test execution to reduce build time üìà Integration of performance testing üîê Security testing using GitHub CodeQL üí∞ Cost tracking and AWS resource optimization This is a hands-on workshop, highly practical, designed for:\nIT students working on final projects or internships Developers who want to adopt AWS-based CI/CD DevOps interns looking for real-world testing pipeline experience üöÄ Get ready to elevate your DevOps skills with AWS! "
},
{
	"uri": "//localhost:1313/2-environment-setup/2.1-dotnet-app/",
	"title": ".NET Application Setup",
	"tags": [],
	"description": "",
	"content": "Setting up the .NET Application Overview In this section, we will create a new .NET 8 MVC application that will serve as our test project throughout the workshop. We\u0026rsquo;ll set up a basic application with the necessary dependencies for automated testing.\nPrerequisites .NET 8 SDK installed on your machine Basic understanding of C# and .NET MVC A code editor (Visual Studio Code recommended) Implementation Steps 1. Verify .NET Installation First, verify that you have .NET 8 SDK installed:\ndotnet --version The output should show version 8.0.x\n2. Create the MVC Application Create a new directory and initialize the MVC application:\nmkdir TestAutomationDemo cd TestAutomationDemo dotnet new mvc 3. Add Required Packages Install the necessary NuGet packages for testing:\ndotnet add package xunit --version 2.5.0 dotnet add package xunit.runner.visualstudio --version 2.5.0 dotnet add package Microsoft.NET.Test.Sdk --version 17.7.0 4. Create Sample Controller Add a new controller for testing purposes:\nusing Microsoft.AspNetCore.Mvc; namespace TestAutomationDemo.Controllers { public class CalculatorController : Controller { [HttpGet(\u0026#34;api/calculator/add/{a}/{b}\u0026#34;)] public IActionResult Add(int a, int b) { return Ok(new { result = a + b }); } [HttpGet(\u0026#34;api/calculator/multiply/{a}/{b}\u0026#34;)] public IActionResult Multiply(int a, int b) { return Ok(new { result = a * b }); } } } 5. Test the Application Build and run the application:\ndotnet build dotnet run Access the application at:\nMain page: https://localhost:5001 Calculator API: https://localhost:5001/api/calculator/add/5/3 Verification Steps Ensure the application builds without errors Verify you can access the homepage Test the calculator endpoints using a browser or Postman Confirm all NuGet packages are properly installed Common Issues and Solutions If port 5001 is in use, modify Properties/launchSettings.json If SSL certificate issues occur, run dotnet dev-certs https --trust For package restore issues, try dotnet restore --force Next Steps Once your .NET application is running successfully, proceed to Setting up GitHub Repository.\n"
},
{
	"uri": "//localhost:1313/5-performance-testing/5.4-analyze-performance/",
	"title": "Create VPC",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/6-security-testing/6.1-enable-codeql/",
	"title": "Create VPC",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/6-security-testing/6.2-review-alerts/",
	"title": "Create VPC",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/6-security-testing/6.3-fix-vulnerabilities/",
	"title": "Create VPC",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/6-security-testing/6.4-disable-if-needed/",
	"title": "Create VPC",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/7-monitoring-cost/7.1-cloudwatch-logs/",
	"title": "Create VPC",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/7-monitoring-cost/7.2-analyze-cost/",
	"title": "Create VPC",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/7-monitoring-cost/7.3-optimize-config/",
	"title": "Create VPC",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/7-monitoring-cost/7.4-estimate-usage/",
	"title": "Create VPC",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/4-parallel-execution/4.1-multiple-tests/",
	"title": "Multiple Tests Setup",
	"tags": [],
	"description": "",
	"content": "Setting Up Multiple Test Projects Overview In this section, we\u0026rsquo;ll organize our test projects to prepare for parallel execution. We\u0026rsquo;ll create different test categories and configure them for efficient parallel processing.\nImplementation Steps 1. Create Test Projects Structure Create test project directories: mkdir -p TestAutomationDemo.Tests.Unit mkdir -p TestAutomationDemo.Tests.Integration mkdir -p TestAutomationDemo.Tests.Performance Initialize test projects: # Unit Tests Project dotnet new xunit -n TestAutomationDemo.Tests.Unit dotnet add TestAutomationDemo.Tests.Unit/TestAutomationDemo.Tests.Unit.csproj reference TestAutomationDemo.csproj # Integration Tests Project dotnet new xunit -n TestAutomationDemo.Tests.Integration dotnet add TestAutomationDemo.Tests.Integration/TestAutomationDemo.Tests.Integration.csproj reference TestAutomationDemo.csproj # Performance Tests Project dotnet new xunit -n TestAutomationDemo.Tests.Performance dotnet add TestAutomationDemo.Tests.Performance/TestAutomationDemo.Tests.Performance.csproj reference TestAutomationDemo.csproj 2. Configure Test Categories Unit Tests Setup: // TestAutomationDemo.Tests.Unit/CalculatorTests.cs namespace TestAutomationDemo.Tests.Unit { [Trait(\u0026#34;Category\u0026#34;, \u0026#34;Unit\u0026#34;)] [Trait(\u0026#34;Priority\u0026#34;, \u0026#34;1\u0026#34;)] public class CalculatorTests { [Fact] public void Add_ValidNumbers_ReturnsSum() { // Test implementation } } } Integration Tests Setup: // TestAutomationDemo.Tests.Integration/ApiTests.cs namespace TestAutomationDemo.Tests.Integration { [Trait(\u0026#34;Category\u0026#34;, \u0026#34;Integration\u0026#34;)] [Trait(\u0026#34;Priority\u0026#34;, \u0026#34;2\u0026#34;)] public class ApiTests { [Fact] public async Task Api_ReturnsExpectedResponse() { // Test implementation } } } Performance Tests Setup: // TestAutomationDemo.Tests.Performance/LoadTests.cs namespace TestAutomationDemo.Tests.Performance { [Trait(\u0026#34;Category\u0026#34;, \u0026#34;Performance\u0026#34;)] [Trait(\u0026#34;Priority\u0026#34;, \u0026#34;3\u0026#34;)] public class LoadTests { [Fact] public async Task Api_HandlesMultipleRequests() { // Test implementation } } } 3. Configure Test Settings Create test settings file: // testsettings.json { \u0026#34;TestSettings\u0026#34;: { \u0026#34;Parallelization\u0026#34;: { \u0026#34;MaxCpuCount\u0026#34;: 0, \u0026#34;Workers\u0026#34;: 3, \u0026#34;Scope\u0026#34;: \u0026#34;ClassLevel\u0026#34; }, \u0026#34;Categories\u0026#34;: { \u0026#34;Unit\u0026#34;: { \u0026#34;Priority\u0026#34;: 1, \u0026#34;Timeout\u0026#34;: \u0026#34;00:01:00\u0026#34; }, \u0026#34;Integration\u0026#34;: { \u0026#34;Priority\u0026#34;: 2, \u0026#34;Timeout\u0026#34;: \u0026#34;00:05:00\u0026#34; }, \u0026#34;Performance\u0026#34;: { \u0026#34;Priority\u0026#34;: 3, \u0026#34;Timeout\u0026#34;: \u0026#34;00:10:00\u0026#34; } } } } Update project files: \u0026lt;!-- Directory.Build.props --\u0026gt; \u0026lt;Project\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;VSTestLogger\u0026gt;trx\u0026lt;/VSTestLogger\u0026gt; \u0026lt;VSTestResultsDirectory\u0026gt;$(MSBuildProjectDirectory)/TestResults\u0026lt;/VSTestResultsDirectory\u0026gt; \u0026lt;CollectCoverage\u0026gt;true\u0026lt;/CollectCoverage\u0026gt; \u0026lt;CoverletOutputFormat\u0026gt;cobertura\u0026lt;/CoverletOutputFormat\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;/Project\u0026gt; 4. Create Test Run Scripts Create PowerShell script: # run-tests.ps1 param( [string]$Category = \u0026#34;All\u0026#34;, [int]$Priority = 0 ) $testProjects = Get-ChildItem -Filter \u0026#34;*.Tests.*\u0026#34; -Directory foreach ($project in $testProjects) { if ($Category -eq \u0026#34;All\u0026#34; -or $project.Name -match $Category) { Write-Host \u0026#34;Running tests for $($project.Name)\u0026#34; dotnet test $project.FullName ` --filter \u0026#34;Priority=$Priority\u0026#34; ` --logger \u0026#34;trx;LogFileName=$($project.Name).trx\u0026#34; } } Create bash script: #!/bin/bash # run-tests.sh category=${1:-\u0026#34;All\u0026#34;} priority=${2:-0} for project in $(find . -name \u0026#34;*.Tests.*\u0026#34; -type d); do if [ \u0026#34;$category\u0026#34; = \u0026#34;All\u0026#34; ] || [[ $project =~ $category ]]; then echo \u0026#34;Running tests for $project\u0026#34; dotnet test \u0026#34;$project\u0026#34; \\ --filter \u0026#34;Priority=$priority\u0026#34; \\ --logger \u0026#34;trx;LogFileName=$(basename $project).trx\u0026#34; fi done Test Organization Best Practices Project Structure\nSeparate test types Clear naming conventions Logical grouping Test Categories\nMeaningful traits Priority levels Execution order Configuration\nCentralized settings Environment-specific configs Reusable properties Verification Steps Verify project structure: dotnet sln list Run specific category: dotnet test --filter \u0026#34;Category=Unit\u0026#34; Check test discovery: dotnet test -t Next Steps After setting up multiple test projects, proceed to Configure Parallel Execution to learn how to run these tests in parallel.\n"
},
{
	"uri": "//localhost:1313/5-performance-testing/5.1-write-performance-tests/",
	"title": "Write Performance Tests",
	"tags": [],
	"description": "",
	"content": "Writing Performance Tests Overview In this section, we\u0026rsquo;ll create performance tests using k6, a modern load testing tool. We\u0026rsquo;ll write tests to measure response times, throughput, and resource utilization of our .NET application.\nImplementation Steps 1. Install k6 Install on Linux: sudo gpg -k sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69 echo \u0026#34;deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main\u0026#34; | sudo tee /etc/apt/sources.list.d/k6.list sudo apt-get update sudo apt-get install k6 Install on Windows: winget install k6 2. Create Basic Load Test Create test script: // basic-load-test.js import http from \u0026#39;k6/http\u0026#39;; import { check, sleep } from \u0026#39;k6\u0026#39;; export const options = { stages: [ { duration: \u0026#39;30s\u0026#39;, target: 20 }, // Ramp up to 20 users { duration: \u0026#39;1m\u0026#39;, target: 20 }, // Stay at 20 users { duration: \u0026#39;30s\u0026#39;, target: 0 }, // Ramp down to 0 users ], thresholds: { http_req_duration: [\u0026#39;p(95)\u0026lt;500\u0026#39;], // 95% of requests should be below 500ms http_req_failed: [\u0026#39;rate\u0026lt;0.01\u0026#39;], // Less than 1% can fail }, }; export default function () { const BASE_URL = \u0026#39;https://localhost:5001\u0026#39;; // Test calculator endpoints const responses = http.batch([ [\u0026#39;GET\u0026#39;, `${BASE_URL}/api/calculator/add/5/3`], [\u0026#39;GET\u0026#39;, `${BASE_URL}/api/calculator/multiply/4/6`], ]); // Check responses check(responses[0], { \u0026#39;addition status is 200\u0026#39;: (r) =\u0026gt; r.status === 200, \u0026#39;addition result is correct\u0026#39;: (r) =\u0026gt; JSON.parse(r.body).result === 8, }); check(responses[1], { \u0026#39;multiplication status is 200\u0026#39;: (r) =\u0026gt; r.status === 200, \u0026#39;multiplication result is correct\u0026#39;: (r) =\u0026gt; JSON.parse(r.body).result === 24, }); sleep(1); } 3. Create Stress Test Create stress test script: // stress-test.js import http from \u0026#39;k6/http\u0026#39;; import { check, sleep } from \u0026#39;k6\u0026#39;; export const options = { stages: [ { duration: \u0026#39;2m\u0026#39;, target: 100 }, // Ramp up to 100 users { duration: \u0026#39;5m\u0026#39;, target: 100 }, // Stay at 100 users { duration: \u0026#39;2m\u0026#39;, target: 200 }, // Ramp up to 200 users { duration: \u0026#39;5m\u0026#39;, target: 200 }, // Stay at 200 users { duration: \u0026#39;2m\u0026#39;, target: 0 }, // Ramp down to 0 users ], thresholds: { http_req_duration: [\u0026#39;p(99)\u0026lt;1500\u0026#39;], // 99% of requests should be below 1.5s http_req_failed: [\u0026#39;rate\u0026lt;0.02\u0026#39;], // Less than 2% can fail }, }; export default function () { const BASE_URL = \u0026#39;https://localhost:5001\u0026#39;; // Simulate complex calculations const randomNumbers = Array.from( { length: 10 }, () =\u0026gt; Math.floor(Math.random() * 100) ); // Batch requests const requests = randomNumbers.map((num, index) =\u0026gt; { return [\u0026#39;GET\u0026#39;, `${BASE_URL}/api/calculator/multiply/${num}/${index + 1}`]; }); const responses = http.batch(requests); // Check responses responses.forEach((response, index) =\u0026gt; { check(response, { \u0026#39;status is 200\u0026#39;: (r) =\u0026gt; r.status === 200, \u0026#39;response is valid\u0026#39;: (r) =\u0026gt; JSON.parse(r.body).result !== undefined, }); }); sleep(1); } 4. Create Performance Metrics Collection Create metrics script: // collect-metrics.js import { Counter, Gauge, Rate } from \u0026#39;k6/metrics\u0026#39;; import http from \u0026#39;k6/http\u0026#39;; // Custom metrics const successfulCalculations = new Counter(\u0026#39;successful_calculations\u0026#39;); const failedCalculations = new Counter(\u0026#39;failed_calculations\u0026#39;); const responseTime = new Gauge(\u0026#39;response_time\u0026#39;); const successRate = new Rate(\u0026#39;success_rate\u0026#39;); export const options = { scenarios: { constant_load: { executor: \u0026#39;constant-vus\u0026#39;, vus: 10, duration: \u0026#39;5m\u0026#39;, }, }, }; export default function () { const BASE_URL = \u0026#39;https://localhost:5001\u0026#39;; // Record response time const startTime = new Date(); const response = http.get(`${BASE_URL}/api/calculator/add/5/3`); const endTime = new Date(); // Update metrics responseTime.add(endTime - startTime); successRate.add(response.status === 200); if (response.status === 200) { successfulCalculations.add(1); } else { failedCalculations.add(1); } } Best Practices Test Design\nStart with realistic scenarios Include ramp-up periods Set appropriate thresholds Monitor system resources Test Data\nUse representative data Avoid static values Consider data cleanup Handle test isolation Error Handling\nSet failure criteria Log detailed errors Monitor error rates Implement retries Verification Steps Run basic load test: k6 run basic-load-test.js Run stress test: k6 run stress-test.js Collect metrics: k6 run --out json=metrics.json collect-metrics.js Next Steps After writing performance tests, proceed to Integrate into CI/CD to automate test execution in your pipeline.\n"
},
{
	"uri": "//localhost:1313/3-automated-unit-test/3.1-write-unit-tests/",
	"title": "Writing Unit Tests",
	"tags": [],
	"description": "",
	"content": "Writing Unit Tests Overview In this section, we\u0026rsquo;ll create unit tests for our .NET application using xUnit. We\u0026rsquo;ll focus on writing effective, maintainable tests that provide good coverage of our application logic.\nImplementation Steps 1. Create Test Project # Navigate to solution directory cd TestAutomationDemo # Create test project dotnet new xunit -n TestAutomationDemo.Tests # Add project reference dotnet add TestAutomationDemo.Tests/TestAutomationDemo.Tests.csproj reference TestAutomationDemo.csproj 2. Add Test Dependencies cd TestAutomationDemo.Tests dotnet add package xunit --version 2.5.0 dotnet add package xunit.runner.visualstudio --version 2.5.0 dotnet add package Microsoft.NET.Test.Sdk --version 17.7.0 dotnet add package Moq --version 4.20.0 3. Create Test Structure // CalculatorControllerTests.cs using Xunit; using Microsoft.AspNetCore.Mvc; using TestAutomationDemo.Controllers; namespace TestAutomationDemo.Tests { public class CalculatorControllerTests { private readonly CalculatorController _controller; public CalculatorControllerTests() { _controller = new CalculatorController(); } [Fact] public void Add_ValidNumbers_ReturnsCorrectSum() { // Arrange int a = 5; int b = 3; // Act var result = _controller.Add(a, b) as OkObjectResult; var value = (dynamic)result?.Value; // Assert Assert.NotNull(result); Assert.Equal(200, result.StatusCode); Assert.Equal(8, value.result); } [Fact] public void Multiply_ValidNumbers_ReturnsCorrectProduct() { // Arrange int a = 4; int b = 5; // Act var result = _controller.Multiply(a, b) as OkObjectResult; var value = (dynamic)result?.Value; // Assert Assert.NotNull(result); Assert.Equal(200, result.StatusCode); Assert.Equal(20, value.result); } [Theory] [InlineData(0, 5, 0)] [InlineData(1, 0, 0)] [InlineData(10, 1, 10)] public void Multiply_SpecialCases_ReturnsExpectedResult(int a, int b, int expected) { // Act var result = _controller.Multiply(a, b) as OkObjectResult; var value = (dynamic)result?.Value; // Assert Assert.Equal(expected, value.result); } } } 4. Add Test Categories [Trait(\u0026#34;Category\u0026#34;, \u0026#34;Unit\u0026#34;)] public class CalculatorControllerTests { // ... existing test methods ... } 5. Run Tests Locally # Run all tests dotnet test # Run specific category dotnet test --filter \u0026#34;Category=Unit\u0026#34; # Run with detailed output dotnet test -l \u0026#34;console;verbosity=detailed\u0026#34; Best Practices Test Naming Convention\nMethodName_Scenario_ExpectedBehavior Clear and descriptive names Indicates what\u0026rsquo;s being tested Test Structure\nArrange: Set up test data Act: Execute method being tested Assert: Verify results Test Coverage\nTest positive scenarios Test edge cases Test error conditions Code Organization\nOne test class per production class Group related tests Use appropriate attributes Common Testing Patterns AAA Pattern (Arrange-Act-Assert) [Fact] public void Method_Scenario_Expected() { // Arrange var setup = new Setup(); // Act var result = setup.Execute(); // Assert Assert.Equal(expected, result); } Theory Tests [Theory] [InlineData(input1, expected1)] [InlineData(input2, expected2)] public void Method_MultipleScenarios(input, expected) { var result = Method(input); Assert.Equal(expected, result); } Verification Steps All tests pass locally Test coverage is adequate Tests are properly organized Naming conventions are followed Edge cases are covered Next Steps Once your unit tests are written and passing locally, proceed to BuildSpec Setup to configure automated test execution in AWS CodeBuild.\n"
},
{
	"uri": "//localhost:1313/3-automated-unit-test/3.2-buildspec-setup/",
	"title": "BuildSpec Setup",
	"tags": [],
	"description": "",
	"content": "BuildSpec Configuration for Unit Testing Overview In this section, we\u0026rsquo;ll configure the buildspec.yml file to automate unit test execution in AWS CodeBuild. The buildspec file will define how tests are run, reported, and integrated into the CI/CD pipeline.\nImplementation Steps 1. Create buildspec.yml Create or update the buildspec.yml file in your repository root:\nversion: 0.2 phases: install: runtime-versions: dotnet: 8.0 commands: - echo Installing test dependencies... - dotnet tool install -g dotnet-reportgenerator-globaltool pre_build: commands: - echo Restore started on `date` - dotnet restore build: commands: - echo Build started on `date` - dotnet build -c Release post_build: commands: - echo Test started on `date` - dotnet test --no-build -c Release --logger \u0026#34;trx;LogFileName=testresults.trx\u0026#34; --collect:\u0026#34;XPlat Code Coverage\u0026#34; - reportgenerator -reports:\u0026#34;**/coverage.cobertura.xml\u0026#34; -targetdir:\u0026#34;coveragereport\u0026#34; -reporttypes:Html reports: dotnet-tests: file-format: VisualStudioTrx files: - \u0026#39;**/*.trx\u0026#39; base-directory: \u0026#39;$CODEBUILD_SRC_DIR\u0026#39; coverage-report: files: - \u0026#39;coveragereport/**/*\u0026#39; base-directory: \u0026#39;$CODEBUILD_SRC_DIR\u0026#39; file-format: HTML artifacts: files: - \u0026#39;**/*\u0026#39; base-directory: \u0026#39;$CODEBUILD_SRC_DIR\u0026#39; 2. Configure Test Settings Add test settings to your test project:\n\u0026lt;!-- TestAutomationDemo.Tests.csproj --\u0026gt; \u0026lt;Project Sdk=\u0026#34;Microsoft.NET.Sdk\u0026#34;\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;TargetFramework\u0026gt;net8.0\u0026lt;/TargetFramework\u0026gt; \u0026lt;IsPackable\u0026gt;false\u0026lt;/IsPackable\u0026gt; \u0026lt;CollectCoverage\u0026gt;true\u0026lt;/CollectCoverage\u0026gt; \u0026lt;CoverletOutputFormat\u0026gt;cobertura\u0026lt;/CoverletOutputFormat\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;PackageReference Include=\u0026#34;coverlet.collector\u0026#34; Version=\u0026#34;6.0.0\u0026#34;\u0026gt; \u0026lt;IncludeAssets\u0026gt;runtime; build; native; contentfiles; analyzers\u0026lt;/IncludeAssets\u0026gt; \u0026lt;PrivateAssets\u0026gt;all\u0026lt;/PrivateAssets\u0026gt; \u0026lt;/PackageReference\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;/Project\u0026gt; 3. Configure Test Thresholds Add code coverage thresholds:\n\u0026lt;PropertyGroup\u0026gt; \u0026lt;VSTestLogger\u0026gt;trx\u0026lt;/VSTestLogger\u0026gt; \u0026lt;VSTestResultsDirectory\u0026gt;$(MSBuildProjectDirectory)/TestResults\u0026lt;/VSTestResultsDirectory\u0026gt; \u0026lt;CoverletThreshold\u0026gt;80\u0026lt;/CoverletThreshold\u0026gt; \u0026lt;/PropertyGroup\u0026gt; 4. Add Test Filters Configure test filtering in buildspec.yml:\npost_build: commands: - dotnet test --no-build -c Release --filter \u0026#34;Category=Unit\u0026#34; --logger \u0026#34;trx;LogFileName=testresults.trx\u0026#34; BuildSpec Components Explained Phases\ninstall: Set up .NET and tools pre_build: Restore dependencies build: Compile the solution post_build: Run tests and generate reports Reports\nTest results in TRX format Code coverage in HTML format Configurable report locations Artifacts\nTest results Coverage reports Build outputs Test Report Configuration TRX Logger --logger \u0026#34;trx;LogFileName=testresults.trx\u0026#34; Coverage Report reportgenerator -reports:\u0026#34;**/coverage.cobertura.xml\u0026#34; -targetdir:\u0026#34;coveragereport\u0026#34; -reporttypes:Html Best Practices Build Configuration\nUse Release configuration Enable deterministic builds Set appropriate timeouts Test Execution\nRun tests in parallel when possible Filter tests appropriately Collect relevant metrics Report Generation\nUse standard formats (TRX, Cobertura) Generate human-readable reports Store results consistently Common Issues and Solutions Build Failures\nCheck .NET version compatibility Verify package references Review build logs Test Execution Issues\nIncrease timeout values Check test discovery Verify test dependencies Report Generation Problems\nCheck file permissions Verify tool installation Review path configurations Next Steps After configuring your buildspec.yml, proceed to Push and Trigger to test the automated execution.\n"
},
{
	"uri": "//localhost:1313/4-parallel-execution/4.2-configure-parallel/",
	"title": "Configure Parallel Execution",
	"tags": [],
	"description": "",
	"content": "Configuring Parallel Test Execution Overview In this section, we\u0026rsquo;ll configure AWS CodeBuild and our test projects to execute tests in parallel, maximizing resource utilization and reducing overall test execution time.\nImplementation Steps 1. Configure CodeBuild for Parallel Execution Update buildspec.yml for parallel execution: version: 0.2 batch: fast-fail: false build-graph: - identifier: UnitTests env: compute-type: BUILD_GENERAL1_SMALL variables: TEST_CATEGORY: \u0026#34;Unit\u0026#34; buildspec: unit-buildspec.yml - identifier: IntegrationTests env: compute-type: BUILD_GENERAL1_MEDIUM variables: TEST_CATEGORY: \u0026#34;Integration\u0026#34; buildspec: integration-buildspec.yml - identifier: PerformanceTests env: compute-type: BUILD_GENERAL1_LARGE variables: TEST_CATEGORY: \u0026#34;Performance\u0026#34; buildspec: performance-buildspec.yml Create category-specific buildspec files: # unit-buildspec.yml version: 0.2 phases: install: runtime-versions: dotnet: 8.0 build: commands: - dotnet test TestAutomationDemo.Tests.Unit/TestAutomationDemo.Tests.Unit.csproj --filter \u0026#34;Category=Unit\u0026#34; --logger \u0026#34;trx;LogFileName=unit-tests.trx\u0026#34; --results-directory /test-results artifacts: files: - /test-results/**/* name: unit-test-results 2. Configure Test Parallelization Update test project settings: \u0026lt;!-- TestAutomationDemo.Tests.Unit/TestAutomationDemo.Tests.Unit.csproj --\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;ParallelizeTestCollections\u0026gt;true\u0026lt;/ParallelizeTestCollections\u0026gt; \u0026lt;MaxParallelThreads\u0026gt;4\u0026lt;/MaxParallelThreads\u0026gt; \u0026lt;/PropertyGroup\u0026gt; Configure test assembly attributes: // AssemblyInfo.cs using Xunit; [assembly: CollectionBehavior(DisableTestParallelization = false, MaxParallelThreads = 4)] 3. Resource Allocation Create resource configuration: // parallel-config.json { \u0026#34;computeTypes\u0026#34;: { \u0026#34;UnitTests\u0026#34;: { \u0026#34;compute\u0026#34;: \u0026#34;BUILD_GENERAL1_SMALL\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;3GB\u0026#34;, \u0026#34;cpu\u0026#34;: \u0026#34;2 vCPU\u0026#34; }, \u0026#34;IntegrationTests\u0026#34;: { \u0026#34;compute\u0026#34;: \u0026#34;BUILD_GENERAL1_MEDIUM\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;7GB\u0026#34;, \u0026#34;cpu\u0026#34;: \u0026#34;4 vCPU\u0026#34; }, \u0026#34;PerformanceTests\u0026#34;: { \u0026#34;compute\u0026#34;: \u0026#34;BUILD_GENERAL1_LARGE\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;15GB\u0026#34;, \u0026#34;cpu\u0026#34;: \u0026#34;8 vCPU\u0026#34; } } } Update AWS CodeBuild project: aws codebuild update-project \\ --name dotnet-test-automation \\ --concurrent-build-limit 3 4. Environment Isolation Create Docker compose file: # docker-compose.test.yml version: \u0026#39;3.8\u0026#39; services: unit-tests: build: context: . dockerfile: Dockerfile.tests environment: - TEST_CATEGORY=Unit volumes: - ./test-results:/test-results integration-tests: build: context: . dockerfile: Dockerfile.tests environment: - TEST_CATEGORY=Integration volumes: - ./test-results:/test-results performance-tests: build: context: . dockerfile: Dockerfile.tests environment: - TEST_CATEGORY=Performance volumes: - ./test-results:/test-results Monitoring Configuration Set up CloudWatch metrics: aws cloudwatch put-metric-alarm \\ --alarm-name ParallelExecutionDuration \\ --metric-name Duration \\ --namespace AWS/CodeBuild \\ --statistic Average \\ --period 300 \\ --threshold 1800 \\ --comparison-operator GreaterThanThreshold Configure logging: phases: build: commands: - | echo \u0026#34;Starting parallel execution at $(date)\u0026#34; dotnet test | tee test-output.log echo \u0026#34;Completed parallel execution at $(date)\u0026#34; Best Practices Resource Management\nAllocate resources based on test type Monitor resource utilization Implement timeout policies Test Independence\nEnsure tests can run in any order Avoid shared state Clean up test data Error Handling\nConfigure failure thresholds Implement retry logic Log detailed error information Common Issues and Solutions Resource Constraints\nMonitor memory usage Adjust compute types Implement test batching Test Interference\nIsolate test environments Use unique test data Implement cleanup routines Timing Issues\nAdd appropriate wait conditions Implement retry mechanisms Log detailed timing information Next Steps After configuring parallel execution, proceed to Aggregate Results to learn how to combine and analyze results from parallel test runs.\n"
},
{
	"uri": "//localhost:1313/2-environment-setup/",
	"title": "Environment Setup",
	"tags": [],
	"description": "",
	"content": "Environment Setup Overview This module guides you through setting up your development and testing environment for automated testing with AWS CodeBuild. We\u0026rsquo;ll establish all necessary components to create a robust continuous integration pipeline.\nWhat You\u0026rsquo;ll Learn Setting up a .NET Application\nCreating a new .NET 8 MVC project Configuring basic application structure Adding initial test dependencies GitHub Repository Configuration\nCreating and initializing repository Setting up branch protection Configuring development workflow Installing Required Tools\nAWS CLI installation and configuration Development tools setup Required SDK installations CodeBuild Project Setup\nCreating AWS CodeBuild project Configuring build specifications Setting up IAM roles and permissions Webhook Integration\nConfiguring GitHub webhooks Testing automated triggers Verifying build pipeline Prerequisites Before starting this module, ensure you have:\nAWS Account with appropriate permissions Basic knowledge of Git and GitHub Understanding of .NET development Administrative access to your development machine Time Estimation Total Module Time: ~2 hours Individual Section Time: 20-30 minutes each Module Structure .NET Application Setup\nBasic application setup Initial configuration GitHub Repository Setup\nRepository creation Branch configuration Required Tools Installation\nDevelopment environment setup AWS tools installation CodeBuild Project Creation\nAWS CodeBuild configuration Build pipeline setup Webhook Verification\nIntegration testing Automation verification Expected Outcomes By the end of this module, you will have:\nA functioning .NET application A configured GitHub repository All necessary development tools installed An operational AWS CodeBuild project Automated build triggers through webhooks Let\u0026rsquo;s begin with .NET Application Setup!\n"
},
{
	"uri": "//localhost:1313/2-environment-setup/2.2-github-repo/",
	"title": "GitHub Repository Setup",
	"tags": [],
	"description": "",
	"content": "Setting up the GitHub Repository Overview In this section, we\u0026rsquo;ll create a GitHub repository for our .NET application and set up the necessary configurations for AWS CodeBuild integration.\nPrerequisites GitHub account Git installed locally .NET application from previous section Implementation Steps 1. Create GitHub Repository Go to GitHub Click \u0026ldquo;New\u0026rdquo; to create a new repository Name: dotnet-test-automation Description: \u0026ldquo;AWS CodeBuild Test Automation Demo\u0026rdquo; Choose \u0026ldquo;Public\u0026rdquo; repository Don\u0026rsquo;t initialize with README (we\u0026rsquo;ll push our existing code) 2. Initialize Local Git Repository cd TestAutomationDemo git init git branch -M main 3. Create .gitignore File dotnet new gitignore 4. Configure Repository git remote add origin https://github.com/\u0026lt;your-username\u0026gt;/dotnet-test-automation.git 5. Initial Commit git add . git commit -m \u0026#34;Initial commit: Basic .NET MVC application\u0026#34; git push -u origin main 6. Create Development Branch git checkout -b development git push -u origin development 7. Configure Branch Protection Go to repository Settings \u0026gt; Branches Add branch protection rule for main Require pull request reviews Require status checks to pass Include administrators in restrictions Repository Structure dotnet-test-automation/ ‚îú‚îÄ‚îÄ Controllers/ ‚îú‚îÄ‚îÄ Models/ ‚îú‚îÄ‚îÄ Views/ ‚îú‚îÄ‚îÄ wwwroot/ ‚îú‚îÄ‚îÄ .gitignore ‚îú‚îÄ‚îÄ Program.cs ‚îú‚îÄ‚îÄ TestAutomationDemo.csproj ‚îî‚îÄ‚îÄ appsettings.json Branch Strategy main: Production-ready code development: Integration branch Feature branches: Created from development Verification Steps Verify repository creation on GitHub Confirm all files are pushed Test branch protection rules Validate .gitignore effectiveness Common Issues and Solutions If push fails, check remote URL configuration For permission issues, verify GitHub authentication If files are missing, check .gitignore settings Next Steps Once your GitHub repository is set up, proceed to Installing Required Tools.\n"
},
{
	"uri": "//localhost:1313/5-performance-testing/5.2-integrate-into-ci/",
	"title": "Integrate into CI/CD",
	"tags": [],
	"description": "",
	"content": "Integrating Performance Tests into CI/CD Overview In this section, we\u0026rsquo;ll integrate our performance tests into the AWS CodeBuild CI/CD pipeline. We\u0026rsquo;ll configure the build process to automatically run performance tests and evaluate the results.\nImplementation Steps 1. Update BuildSpec Configuration Create performance test buildspec: # performance-buildspec.yml version: 0.2 phases: install: runtime-versions: nodejs: 18 commands: - curl https://github.com/grafana/k6/releases/download/v0.45.0/k6-v0.45.0-linux-amd64.tar.gz -L | tar xvz --strip-components=1 - mv k6 /usr/local/bin pre_build: commands: - echo Starting performance tests at $(date) - mkdir -p performance-results build: commands: - k6 run basic-load-test.js --out json=performance-results/load-test.json - k6 run stress-test.js --out json=performance-results/stress-test.json - k6 run collect-metrics.js --out json=performance-results/metrics.json post_build: commands: - echo Performance tests completed at $(date) - node generate-performance-report.js reports: performance-reports: files: - performance-results/*.json file-format: JSON artifacts: files: - performance-results/**/* - performance-report.html name: performance-test-results-$(date +%Y-%m-%d) 2. Create Performance Test Job Configure CodeBuild project: aws codebuild create-project \\ --name performance-test-project \\ --source type=GITHUB,location=https://github.com/\u0026lt;username\u0026gt;/dotnet-test-automation.git \\ --artifacts type=S3,location=performance-test-results \\ --environment type=LINUX_CONTAINER,image=aws/codebuild/standard:7.0,computeType=BUILD_GENERAL1_LARGE \\ --service-role PerformanceTestRole Create IAM role: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;cloudwatch:PutMetricData\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 3. Configure Performance Thresholds Create threshold configuration: // performance-thresholds.js module.exports = { loadTest: { responseTime: { p95: 500, // 95th percentile response time in ms p99: 1000 // 99th percentile response time in ms }, errorRate: 0.01, // 1% error rate threshold throughput: 100 // Minimum requests per second }, stressTest: { responseTime: { p95: 1000, p99: 2000 }, errorRate: 0.02, throughput: 200 } }; Create evaluation script: // evaluate-performance.js const thresholds = require(\u0026#39;./performance-thresholds\u0026#39;); function evaluateResults(results) { const violations = []; if (results.metrics.http_req_duration.p95 \u0026gt; thresholds.loadTest.responseTime.p95) { violations.push(`P95 response time exceeded threshold: ${results.metrics.http_req_duration.p95}ms`); } if (results.metrics.error_rate \u0026gt; thresholds.loadTest.errorRate) { violations.push(`Error rate exceeded threshold: ${results.metrics.error_rate * 100}%`); } return violations; } module.exports = { evaluateResults }; 4. Set Up Monitoring Create CloudWatch dashboard: aws cloudwatch create-dashboard \\ --dashboard-name PerformanceTests \\ --dashboard-body file://performance-dashboard.json Configure metrics: { \u0026#34;metrics\u0026#34;: [ [ \u0026#34;CustomMetrics\u0026#34;, \u0026#34;ResponseTime\u0026#34;, \u0026#34;TestType\u0026#34;, \u0026#34;Load\u0026#34;, { \u0026#34;stat\u0026#34;: \u0026#34;p95\u0026#34; } ], [ \u0026#34;.\u0026#34;, \u0026#34;ErrorRate\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;, { \u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34; } ], [ \u0026#34;.\u0026#34;, \u0026#34;Throughput\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;, { \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34; } ] ], \u0026#34;view\u0026#34;: \u0026#34;timeSeries\u0026#34;, \u0026#34;stacked\u0026#34;: false, \u0026#34;period\u0026#34;: 300 } Best Practices Resource Management\nUse appropriate compute types Monitor resource utilization Clean up test data Implement timeouts Test Scheduling\nRun at consistent times Avoid peak hours Consider time zones Set execution windows Result Handling\nStore historical results Track trends Set alert thresholds Document anomalies Common Issues and Solutions Resource Constraints\nScale compute capacity Optimize test scripts Batch test execution Monitor memory usage Network Issues\nConfigure timeouts Handle retries Log connection errors Monitor bandwidth Data Management\nClean up test data Manage storage Archive results Implement retention Next Steps After integrating performance tests into CI/CD, proceed to Export Results to learn how to collect and analyze test results.\n"
},
{
	"uri": "//localhost:1313/4-parallel-execution/4.3-aggregate-results/",
	"title": "Aggregate Results",
	"tags": [],
	"description": "",
	"content": "Aggregating Parallel Test Results Overview In this section, we\u0026rsquo;ll learn how to collect, combine, and analyze test results from parallel test executions. We\u0026rsquo;ll create a comprehensive reporting system that aggregates results from different test categories and presents them in a meaningful way.\nImplementation Steps 1. Create Result Aggregation Script Create PowerShell aggregation script: # aggregate-results.ps1 param( [string]$ResultsPath = \u0026#34;test-results\u0026#34; ) $totalTests = 0 $passedTests = 0 $failedTests = 0 $skippedTests = 0 Get-ChildItem -Path $ResultsPath -Filter \u0026#34;*.trx\u0026#34; | ForEach-Object { $xml = [xml](Get-Content $_.FullName) $counters = $xml.TestRun.ResultSummary.Counters $totalTests += [int]$counters.total $passedTests += [int]$counters.passed $failedTests += [int]$counters.failed $skippedTests += [int]$counters.skipped } $results = @{ Total = $totalTests Passed = $passedTests Failed = $failedTests Skipped = $skippedTests PassRate = [math]::Round(($passedTests / $totalTests) * 100, 2) } ConvertTo-Json $results \u0026gt; \u0026#34;test-results/summary.json\u0026#34; 2. Configure Result Collection Update buildspec.yml for result collection: version: 0.2 phases: post_build: commands: - mkdir -p combined-results - cp */test-results/*.trx combined-results/ - pwsh ./aggregate-results.ps1 -ResultsPath combined-results reports: test-reports: files: - \u0026#34;**/*.trx\u0026#34; file-format: VisualStudioTrx base-directory: combined-results artifacts: files: - combined-results/**/* - combined-results/summary.json name: test-results-$(date +%Y-%m-%d) 3. Create HTML Report Generator Create report template: \u0026lt;!-- report-template.html --\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Test Execution Summary\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; .summary { margin: 20px; } .passed { color: green; } .failed { color: red; } .chart { width: 600px; height: 400px; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;summary\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Test Execution Summary\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Total Tests: {{totalTests}}\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;passed\u0026#34;\u0026gt;Passed: {{passedTests}}\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;failed\u0026#34;\u0026gt;Failed: {{failedTests}}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Pass Rate: {{passRate}}%\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;chart\u0026#34;\u0026gt; \u0026lt;canvas id=\u0026#34;resultsChart\u0026#34;\u0026gt;\u0026lt;/canvas\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Create report generation script: # generate-report.py import json import jinja2 import matplotlib.pyplot as plt def generate_report(summary_file, template_file, output_file): with open(summary_file) as f: data = json.load(f) # Create pie chart plt.figure(figsize=(8, 8)) plt.pie([data[\u0026#39;Passed\u0026#39;], data[\u0026#39;Failed\u0026#39;], data[\u0026#39;Skipped\u0026#39;]], labels=[\u0026#39;Passed\u0026#39;, \u0026#39;Failed\u0026#39;, \u0026#39;Skipped\u0026#39;], colors=[\u0026#39;green\u0026#39;, \u0026#39;red\u0026#39;, \u0026#39;gray\u0026#39;]) plt.savefig(\u0026#39;test-results/chart.png\u0026#39;) # Generate HTML report with open(template_file) as f: template = jinja2.Template(f.read()) html = template.render( totalTests=data[\u0026#39;Total\u0026#39;], passedTests=data[\u0026#39;Passed\u0026#39;], failedTests=data[\u0026#39;Failed\u0026#39;], passRate=data[\u0026#39;PassRate\u0026#39;] ) with open(output_file, \u0026#39;w\u0026#39;) as f: f.write(html) 4. Upload Results to S3 Create upload script: #!/bin/bash # upload-results.sh BUCKET_NAME=\u0026#34;test-results-bucket\u0026#34; DATE=$(date +%Y-%m-%d) # Upload test results aws s3 cp combined-results s3://$BUCKET_NAME/$DATE/ --recursive # Upload summary report aws s3 cp test-results/report.html \\ s3://$BUCKET_NAME/$DATE/report.html \\ --content-type \u0026#34;text/html\u0026#34; Result Analysis Create analysis script: # analyze-results.py import pandas as pd import numpy as np def analyze_test_results(results_dir): # Read all TRX files dfs = [] for trx_file in glob.glob(f\u0026#34;{results_dir}/*.trx\u0026#34;): df = pd.read_xml(trx_file) dfs.append(df) # Combine results combined_df = pd.concat(dfs) # Calculate statistics stats = { \u0026#39;total_duration\u0026#39;: combined_df[\u0026#39;duration\u0026#39;].sum(), \u0026#39;avg_duration\u0026#39;: combined_df[\u0026#39;duration\u0026#39;].mean(), \u0026#39;success_rate\u0026#39;: (combined_df[\u0026#39;outcome\u0026#39;] == \u0026#39;Passed\u0026#39;).mean() * 100, \u0026#39;failure_patterns\u0026#39;: combined_df[combined_df[\u0026#39;outcome\u0026#39;] == \u0026#39;Failed\u0026#39;][\u0026#39;message\u0026#39;].value_counts() } return stats Best Practices Result Storage\nUse consistent naming conventions Implement retention policies Backup important results Report Generation\nInclude relevant metrics Create visual representations Provide detailed failure information Data Analysis\nTrack trends over time Identify common failure patterns Monitor performance metrics Next Steps After implementing result aggregation, proceed to Compare Speed to analyze the performance improvements from parallel execution.\n"
},
{
	"uri": "//localhost:1313/3-automated-unit-test/",
	"title": "Automated Unit Testing",
	"tags": [],
	"description": "",
	"content": "Automated Unit Testing Overview This module focuses on implementing automated unit testing in our CI/CD pipeline using AWS CodeBuild. You\u0026rsquo;ll learn how to write effective unit tests, configure test automation, and integrate test results into your development workflow.\nWhat You\u0026rsquo;ll Learn Writing Unit Tests\nCreating test projects Writing effective test cases Implementing test patterns Using xUnit framework BuildSpec Configuration\nConfiguring test execution Setting up test reporting Managing test dependencies Push and Trigger Testing\nAutomating test execution Configuring build triggers Managing test workflows Viewing Test Results\nAnalyzing test reports Interpreting test metrics Tracking test coverage Fixing Test Failures\nDebugging failed tests Implementing fixes Validating corrections Prerequisites Before starting this module, ensure you have:\nCompleted Module 2 (Environment Setup) Understanding of C# and .NET testing Familiarity with xUnit framework Access to AWS CodeBuild project Time Estimation Total Module Time: ~2.5 hours Individual Section Time: 30 minutes each Module Structure Writing Unit Tests\nTest project setup Test case implementation BuildSpec Setup\nBuild configuration Test automation setup Push and Trigger\nAutomated execution Pipeline integration View Results\nResult analysis Report interpretation Fix Failures\nTroubleshooting Implementation fixes Expected Outcomes By the end of this module, you will have:\nA comprehensive unit test suite Automated test execution in CI/CD Test result reporting and analysis Experience fixing test failures Improved code quality processes Let\u0026rsquo;s begin with Writing Unit Tests!\n"
},
{
	"uri": "//localhost:1313/5-performance-testing/5.3-export-results/",
	"title": "Export Results",
	"tags": [],
	"description": "",
	"content": "Exporting Performance Test Results Overview In this section, we\u0026rsquo;ll implement mechanisms to export and store performance test results. We\u0026rsquo;ll create a comprehensive data collection system that captures test metrics, generates reports, and stores historical data for trend analysis.\nImplementation Steps 1. Configure Result Export Create result export script: // export-results.js const fs = require(\u0026#39;fs\u0026#39;); const AWS = require(\u0026#39;aws-sdk\u0026#39;); const cloudwatch = new AWS.CloudWatch(); const s3 = new AWS.S3(); async function exportResults(testResults, options) { const timestamp = new Date().toISOString(); const metrics = []; // Format metrics for CloudWatch Object.entries(testResults.metrics).forEach(([key, value]) =\u0026gt; { metrics.push({ MetricName: key, Value: value, Unit: \u0026#39;None\u0026#39;, Timestamp: timestamp, Dimensions: [ { Name: \u0026#39;TestType\u0026#39;, Value: options.testType } ] }); }); // Send metrics to CloudWatch await cloudwatch.putMetricData({ Namespace: \u0026#39;PerformanceTests\u0026#39;, MetricData: metrics }).promise(); // Save detailed results to S3 await s3.putObject({ Bucket: options.resultsBucket, Key: `${options.testType}/${timestamp}.json`, Body: JSON.stringify(testResults), ContentType: \u0026#39;application/json\u0026#39; }).promise(); } module.exports = { exportResults }; 2. Create Results Database Create DynamoDB table: aws dynamodb create-table \\ --table-name PerformanceTestResults \\ --attribute-definitions \\ AttributeName=TestId,AttributeType=S \\ AttributeName=Timestamp,AttributeType=S \\ --key-schema \\ AttributeName=TestId,KeyType=HASH \\ AttributeName=Timestamp,KeyType=RANGE \\ --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 Create database storage script: // store-results.js const AWS = require(\u0026#39;aws-sdk\u0026#39;); const dynamodb = new AWS.DynamoDB.DocumentClient(); async function storeResults(testResults) { const params = { TableName: \u0026#39;PerformanceTestResults\u0026#39;, Item: { TestId: testResults.testId, Timestamp: new Date().toISOString(), TestType: testResults.testType, Duration: testResults.duration, Metrics: { ResponseTime: testResults.metrics.http_req_duration, ErrorRate: testResults.metrics.http_req_failed, Throughput: testResults.metrics.http_reqs }, Environment: testResults.env, Status: testResults.status } }; await dynamodb.put(params).promise(); } 3. Generate Reports Create HTML report template: \u0026lt;!-- report-template.html --\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Performance Test Report\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; .metrics-table { width: 100%; border-collapse: collapse; } .metrics-table th, .metrics-table td { padding: 8px; border: 1px solid #ddd; } .chart-container { width: 800px; height: 400px; margin: 20px 0; } .threshold-violation { color: red; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Performance Test Report\u0026lt;/h1\u0026gt; \u0026lt;div class=\u0026#34;summary\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Test Summary\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Test ID: {{testId}}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Duration: {{duration}}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Status: {{status}}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;metrics\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Key Metrics\u0026lt;/h2\u0026gt; \u0026lt;table class=\u0026#34;metrics-table\u0026#34;\u0026gt; \u0026lt;!-- Metrics data will be inserted here --\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;charts\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Performance Charts\u0026lt;/h2\u0026gt; \u0026lt;!-- Charts will be inserted here --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Create report generation script: // generate-report.js const fs = require(\u0026#39;fs\u0026#39;); const Handlebars = require(\u0026#39;handlebars\u0026#39;); const Chart = require(\u0026#39;chart.js\u0026#39;); async function generateReport(testResults, templatePath, outputPath) { // Read template const template = Handlebars.compile( fs.readFileSync(templatePath, \u0026#39;utf-8\u0026#39;) ); // Generate charts const charts = await generateCharts(testResults); // Create report data const reportData = { testId: testResults.testId, duration: testResults.duration, status: testResults.status, metrics: testResults.metrics, charts: charts }; // Generate HTML const html = template(reportData); fs.writeFileSync(outputPath, html); } 4. Set Up Data Retention Create lifecycle policy: { \u0026#34;Rules\u0026#34;: [ { \u0026#34;ID\u0026#34;: \u0026#34;PerformanceTestRetention\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;Filter\u0026#34;: { \u0026#34;Prefix\u0026#34;: \u0026#34;performance-results/\u0026#34; }, \u0026#34;Transitions\u0026#34;: [ { \u0026#34;Days\u0026#34;: 30, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD_IA\u0026#34; }, { \u0026#34;Days\u0026#34;: 90, \u0026#34;StorageClass\u0026#34;: \u0026#34;GLACIER\u0026#34; } ], \u0026#34;Expiration\u0026#34;: { \u0026#34;Days\u0026#34;: 365 } } ] } Best Practices Data Organization\nUse consistent naming Implement versioning Structure data hierarchically Document data format Storage Management\nImplement retention policies Monitor storage usage Archive old data Backup important results Report Generation\nInclude key metrics Add visualizations Highlight anomalies Provide context Next Steps After setting up result export and storage, proceed to Analyze Performance to learn how to analyze and interpret the collected data.\n"
},
{
	"uri": "//localhost:1313/2-environment-setup/2.3-install-tools/",
	"title": "Install Required Tools",
	"tags": [],
	"description": "",
	"content": "Installing Required Tools Overview In this section, we\u0026rsquo;ll install and configure all the necessary tools needed for our AWS CodeBuild automation project.\nRequired Tools List 1. AWS CLI The AWS Command Line Interface is essential for interacting with AWS services.\n# For Linux curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install # For Windows # Download AWS CLI MSI installer from: https://awscli.amazonaws.com/AWSCLIV2.msi # Verify installation aws --version 2. AWS SAM CLI The AWS Serverless Application Model (SAM) CLI helps with local testing.\n# For Linux wget https://github.com/aws/aws-sam-cli/releases/latest/download/aws-sam-cli-linux-x86_64.zip unzip aws-sam-cli-linux-x86_64.zip -d sam-installation sudo ./sam-installation/install # For Windows # Download SAM CLI MSI installer from AWS website # Verify installation sam --version 3. Configure AWS Credentials Set up your AWS credentials for CLI access:\naws configure # Enter your: # - AWS Access Key ID # - AWS Secret Access Key # - Default region (e.g., us-east-1) # - Default output format (json) 4. Development Tools Visual Studio Code Download from https://code.visualstudio.com/ Install recommended extensions: C# Dev Kit AWS Toolkit GitHub Pull Requests GitLens Git Configuration git config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your.email@example.com\u0026#34; Verification Steps Check AWS CLI Configuration: aws sts get-caller-identity Verify AWS SAM: sam --version Test Git Configuration: git config --list Environment Variables Set up required environment variables:\n# Linux/macOS export AWS_REGION=us-east-1 export PROJECT_NAME=dotnet-test-automation # Windows PowerShell $env:AWS_REGION=\u0026#34;us-east-1\u0026#34; $env:PROJECT_NAME=\u0026#34;dotnet-test-automation\u0026#34; Common Issues and Solutions AWS CLI Issues If AWS CLI commands fail, check credentials in ~/.aws/credentials For permission errors, verify IAM user permissions Region mismatch: ensure default region is set correctly Visual Studio Code Issues If IntelliSense isn\u0026rsquo;t working, reload window For extension issues, try uninstalling and reinstalling Git Issues SSH key problems: verify SSH key is added to GitHub Authentication issues: use GitHub CLI or Personal Access Token Security Best Practices Never commit AWS credentials to Git Use IAM roles with minimum required permissions Regularly rotate access keys Enable MFA for AWS and GitHub accounts Next Steps Once all tools are installed and configured, proceed to Setting up CodeBuild Project.\n"
},
{
	"uri": "//localhost:1313/3-automated-unit-test/3.3-push-trigger/",
	"title": "Push and Trigger",
	"tags": [],
	"description": "",
	"content": "Push Changes and Trigger Tests Overview In this section, we\u0026rsquo;ll learn how to push our changes to GitHub and trigger automated tests in AWS CodeBuild. We\u0026rsquo;ll explore different trigger methods and verify that our CI/CD pipeline executes tests automatically.\nImplementation Steps 1. Prepare Changes Ensure all files are ready: git status Stage changes: git add . git status Commit changes with conventional commit message: git commit -m \u0026#34;test: configure automated unit testing\u0026#34; 2. Push to Different Branches Push to development branch: git checkout development git push origin development Create and push feature branch: git checkout -b feature/unit-tests git push -u origin feature/unit-tests Create Pull Request: gh pr create \\ --title \u0026#34;Add automated unit testing\u0026#34; \\ --body \u0026#34;Implements automated unit testing with AWS CodeBuild\u0026#34; \\ --base development 3. Monitor Build Triggers Check build status using AWS CLI: aws codebuild list-builds-for-project \\ --project-name dotnet-test-automation \\ --sort-order DESCENDING \\ --max-items 5 Get detailed build information: aws codebuild batch-get-builds \\ --ids \u0026lt;build-id\u0026gt; 4. Configure Branch Policies Protected Branches:\nRequire status checks to pass Require up-to-date branches Include administrators in restrictions Branch Protection Rules:\n{ \u0026#34;required_status_checks\u0026#34;: { \u0026#34;strict\u0026#34;: true, \u0026#34;contexts\u0026#34;: [ \u0026#34;AWS CodeBuild dotnet-test-automation\u0026#34; ] }, \u0026#34;enforce_admins\u0026#34;: true } Trigger Types Push Triggers\nDirect pushes to branches Automatically starts builds Configurable branch filters Pull Request Triggers\nNew PR creation PR updates Review submissions Manual Triggers\naws codebuild start-build \\ --project-name dotnet-test-automation \\ --source-version \u0026lt;commit-id\u0026gt; Best Practices Branch Management\nUse feature branches Keep branches short-lived Regular synchronization with main branches Commit Strategy\nClear commit messages Atomic commits Regular small commits CI/CD Integration\nFast feedback loops Automated status checks Clear build status indicators Monitoring and Logging Build Logs aws codebuild get-build-logs \\ --id \u0026lt;build-id\u0026gt; CloudWatch Integration aws logs get-log-events \\ --log-group-name /aws/codebuild/dotnet-test-automation \\ --log-stream-name \u0026lt;build-id\u0026gt; Troubleshooting Guide Failed Triggers\nCheck webhook configuration Verify GitHub permissions Review branch policies Build Failures\nExamine build logs Check resource constraints Verify environment variables Common Issues\nTimeout issues Resource limitations Permission problems Next Steps After successfully triggering your tests, proceed to View Results to learn how to analyze test results and coverage reports.\n"
},
{
	"uri": "//localhost:1313/2-environment-setup/2.4-codebuild-project/",
	"title": "CodeBuild Project Setup",
	"tags": [],
	"description": "",
	"content": "Setting up AWS CodeBuild Project Overview In this section, we\u0026rsquo;ll create and configure an AWS CodeBuild project that will automate our testing process. CodeBuild will be responsible for building our .NET application and running automated tests.\nPrerequisites AWS CLI configured with appropriate permissions GitHub repository set up from previous steps Basic understanding of YAML configuration Implementation Steps 1. Create IAM Role for CodeBuild Create a role that CodeBuild will use to access AWS services:\naws iam create-role --role-name CodeBuildServiceRole --assume-role-policy-document \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;codebuild.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] }\u0026#39; Attach necessary policies:\naws iam attach-role-policy --role-name CodeBuildServiceRole \\ --policy-arn arn:aws:iam::aws:policy/AWSCodeBuildAdminAccess aws iam attach-role-policy --role-name CodeBuildServiceRole \\ --policy-arn arn:aws:iam::aws:policy/CloudWatchLogsFullAccess 2. Create buildspec.yml Create a buildspec.yml file in your repository root:\nversion: 0.2 phases: install: runtime-versions: dotnet: 8.0 pre_build: commands: - echo Restore started on `date` - dotnet restore build: commands: - echo Build started on `date` - dotnet build -c Release post_build: commands: - echo Test started on `date` - dotnet test --no-build -c Release --logger \u0026#34;trx;LogFileName=testresults.trx\u0026#34; reports: dotnet-tests: file-format: VisualStudioTrx files: - \u0026#39;**/*.trx\u0026#39; base-directory: \u0026#39;$CODEBUILD_SRC_DIR\u0026#39; artifacts: files: - \u0026#39;**/*\u0026#39; base-directory: \u0026#39;bin/Release/net8.0/publish\u0026#39; 3. Create CodeBuild Project Using AWS CLI:\naws codebuild create-project \\ --name dotnet-test-automation \\ --source type=GITHUB,location=https://github.com/\u0026lt;username\u0026gt;/dotnet-test-automation.git \\ --source-version main \\ --artifacts type=NO_ARTIFACTS \\ --environment type=LINUX_CONTAINER,image=aws/codebuild/amazonlinux2-x86_64-standard:4.0,computeType=BUILD_GENERAL1_SMALL \\ --service-role CodeBuildServiceRole 4. Configure GitHub Connection Go to AWS CodeBuild Console Select your project Edit Source Connect to GitHub using OAuth Confirm repository access 5. Configure Build Triggers Set up automatic builds:\nGo to Build triggers Enable webhook Configure events: PUSH PULL_REQUEST_CREATED PULL_REQUEST_UPDATED Environment Variables Setup Configure these in CodeBuild project settings:\nDOTNET_ENVIRONMENT: Production ASPNETCORE_ENVIRONMENT: Production TEST_CONFIGURATION: Release Monitoring and Logs View build logs: aws codebuild batch-get-builds --ids \u0026lt;build-id\u0026gt; CloudWatch Logs: Navigate to CloudWatch console Check Log groups: /aws/codebuild/dotnet-test-automation Security Considerations IAM Role Permissions\nUse principle of least privilege Regularly review permissions Remove unused permissions Environment Variables\nUse AWS Secrets Manager for sensitive data Don\u0026rsquo;t store credentials in buildspec.yml Encrypt sensitive environment variables Common Issues and Solutions Build Failures\nCheck buildspec.yml syntax Verify .NET version compatibility Review IAM permissions GitHub Connection Issues\nRefresh OAuth token Check repository permissions Verify webhook configuration Test Execution Problems\nCheck test framework configuration Verify test discovery Review test logging settings Next Steps After setting up CodeBuild, proceed to Webhook Verification to ensure the automation is working correctly.\n"
},
{
	"uri": "//localhost:1313/4-parallel-execution/4.4-compare-speed/",
	"title": "Compare Speed",
	"tags": [],
	"description": "",
	"content": "Comparing Execution Speed Overview In this section, we\u0026rsquo;ll analyze and compare the execution speed between sequential and parallel test runs. We\u0026rsquo;ll measure performance improvements, identify bottlenecks, and optimize test execution for maximum efficiency.\nImplementation Steps 1. Create Performance Measurement Tools Create timing script: # measure-execution.ps1 param( [string]$TestMode = \u0026#34;sequential\u0026#34; ) $stopwatch = [System.Diagnostics.Stopwatch]::StartNew() if ($TestMode -eq \u0026#34;sequential\u0026#34;) { # Run tests sequentially Get-ChildItem -Filter \u0026#34;*.Tests.*\u0026#34; | ForEach-Object { dotnet test $_.FullName } } else { # Run tests in parallel $jobs = Get-ChildItem -Filter \u0026#34;*.Tests.*\u0026#34; | ForEach-Object { Start-Job -ScriptBlock { param($path) dotnet test $path } -ArgumentList $_.FullName } Wait-Job $jobs Receive-Job $jobs } $stopwatch.Stop() $executionTime = $stopwatch.Elapsed return @{ Mode = $TestMode Duration = $executionTime TimeInSeconds = $executionTime.TotalSeconds } 2. Implement Performance Comparison Create comparison script: # compare-performance.py import matplotlib.pyplot as plt import pandas as pd def analyze_performance(sequential_results, parallel_results): # Calculate improvements speedup = sequential_results[\u0026#39;duration\u0026#39;] / parallel_results[\u0026#39;duration\u0026#39;] efficiency = speedup / parallel_results[\u0026#39;worker_count\u0026#39;] # Create comparison chart labels = [\u0026#39;Sequential\u0026#39;, \u0026#39;Parallel\u0026#39;] durations = [sequential_results[\u0026#39;duration\u0026#39;], parallel_results[\u0026#39;duration\u0026#39;]] plt.figure(figsize=(10, 6)) plt.bar(labels, durations) plt.title(\u0026#39;Test Execution Time Comparison\u0026#39;) plt.ylabel(\u0026#39;Duration (seconds)\u0026#39;) # Add improvement annotations plt.annotate(f\u0026#39;Speedup: {speedup:.2f}x\\nEfficiency: {efficiency:.2%}\u0026#39;, xy=(1, parallel_results[\u0026#39;duration\u0026#39;]), xytext=(1.2, parallel_results[\u0026#39;duration\u0026#39;] * 1.2), arrowprops=dict(facecolor=\u0026#39;black\u0026#39;, shrink=0.05)) plt.savefig(\u0026#39;performance-comparison.png\u0026#39;) return { \u0026#39;speedup\u0026#39;: speedup, \u0026#39;efficiency\u0026#39;: efficiency, \u0026#39;time_saved\u0026#39;: sequential_results[\u0026#39;duration\u0026#39;] - parallel_results[\u0026#39;duration\u0026#39;] } 3. Monitor Resource Usage Create resource monitoring script: #!/bin/bash # monitor-resources.sh log_resources() { timestamp=$(date +\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) cpu_usage=$(top -bn1 | grep \u0026#34;Cpu(s)\u0026#34; | awk \u0026#39;{print $2}\u0026#39;) memory_usage=$(free -m | awk \u0026#39;NR==2{printf \u0026#34;%.2f%%\u0026#34;, $3*100/$2}\u0026#39;) echo \u0026#34;$timestamp,$cpu_usage,$memory_usage\u0026#34; \u0026gt;\u0026gt; resource-usage.csv } # Monitor every 5 seconds while true; do log_resources sleep 5 done 4. Generate Performance Reports Create report template: \u0026lt;!-- performance-report-template.html --\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Performance Comparison Report\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; .metrics { margin: 20px; } .improvement { color: green; } .chart { width: 800px; height: 400px; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;metrics\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Performance Metrics\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Sequential Execution: {{sequential_time}} seconds\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Parallel Execution: {{parallel_time}} seconds\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;improvement\u0026#34;\u0026gt;Speed Improvement: {{speedup}}x\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Resource Efficiency: {{efficiency}}%\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;chart\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;performance-comparison.png\u0026#34; alt=\u0026#34;Performance Chart\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Performance Analysis Key Metrics to Monitor:\nTotal execution time CPU utilization Memory usage I/O operations Network bandwidth Bottleneck Identification:\nTest initialization time Resource contention Database operations File system operations Optimization Opportunities:\nTest grouping strategies Resource allocation Cache utilization Data cleanup efficiency Best Practices Performance Measurement\nUse consistent test data Run multiple iterations Account for warm-up time Consider environmental factors Resource Management\nMonitor system resources Optimize resource allocation Handle resource cleanup Prevent resource leaks Reporting\nInclude all relevant metrics Visualize comparisons Document anomalies Track trends over time Common Issues and Solutions Resource Bottlenecks\nImplement resource pooling Optimize test data Use appropriate compute sizes Monitor resource limits Test Dependencies\nMinimize shared resources Implement proper isolation Handle cleanup efficiently Manage test order Performance Degradation\nRegular monitoring Trend analysis Proactive optimization Resource scaling Next Steps After comparing and optimizing test execution speed, proceed to Performance Testing to implement dedicated performance tests.\n"
},
{
	"uri": "//localhost:1313/4-parallel-execution/",
	"title": "Parallel Execution",
	"tags": [],
	"description": "",
	"content": "Parallel Test Execution Overview This module focuses on implementing parallel test execution in AWS CodeBuild to optimize test runtime and improve pipeline efficiency. You\u0026rsquo;ll learn how to configure, manage, and monitor parallel test execution in your CI/CD pipeline.\nWhat You\u0026rsquo;ll Learn Multiple Test Configuration\nOrganizing test projects Test categorization Test prioritization Resource planning Parallel Configuration\nCodeBuild parallel setup Test runner configuration Resource allocation Environment isolation Result Aggregation\nCombining test results Reporting strategies Error handling Status consolidation Performance Comparison\nMeasuring improvements Analyzing bottlenecks Optimizing execution Resource utilization Prerequisites Before starting this module, ensure you have:\nCompleted Module 3 (Automated Unit Testing) Understanding of AWS CodeBuild Familiarity with test execution concepts Basic knowledge of parallel processing Time Estimation Total Module Time: ~2 hours Individual Section Time: 30 minutes each Module Structure Multiple Tests Setup\nTest organization Configuration setup Configure Parallel Execution\nParallel processing setup Resource management Aggregate Results\nResult combination Report generation Compare Speed\nPerformance analysis Optimization techniques Expected Outcomes By the end of this module, you will have:\nConfigured parallel test execution Improved test execution speed Implemented result aggregation Optimized resource usage Measured performance improvements Let\u0026rsquo;s begin with Multiple Tests Setup!\n"
},
{
	"uri": "//localhost:1313/3-automated-unit-test/3.4-view-results/",
	"title": "View Results",
	"tags": [],
	"description": "",
	"content": "Viewing Test Results Overview In this section, we\u0026rsquo;ll explore how to view, analyze, and interpret test results from AWS CodeBuild. We\u0026rsquo;ll learn how to access test reports, understand coverage metrics, and make data-driven decisions about our testing strategy.\nImplementation Steps 1. Access Test Results in AWS Console Navigate to CodeBuild:\nOpen AWS Console Go to CodeBuild Select your project Click on latest build View Test Reports:\nSelect \u0026ldquo;Reports\u0026rdquo; tab Find latest test execution View detailed results 2. Analyze Test Reports via CLI Get build information: aws codebuild batch-get-builds \\ --ids \u0026lt;build-id\u0026gt; \\ --query \u0026#39;builds[].testResults\u0026#39; Get test report details: aws codebuild batch-get-test-reports \\ --report-arns \u0026lt;report-arn\u0026gt; \\ --query \u0026#39;testReports[].testResults\u0026#39; 3. View Coverage Reports Access HTML coverage report: aws s3 cp \\ s3://\u0026lt;bucket-name\u0026gt;/coverage/index.html \\ ./coverage-report.html Analyze coverage metrics: aws codebuild batch-get-reports \\ --reports \u0026lt;report-id\u0026gt; \\ --query \u0026#39;reports[].coveragePercentage\u0026#39; Understanding Test Results Test Status Overview\nTotal Tests Run Passed Tests Failed Tests Skipped Tests Test Duration Coverage Metrics\nLine Coverage Branch Coverage Method Coverage Class Coverage Test Report Structure\n{ \u0026#34;testResults\u0026#34;: { \u0026#34;total\u0026#34;: 25, \u0026#34;passed\u0026#34;: 23, \u0026#34;failed\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 1, \u0026#34;duration\u0026#34;: 3.45, \u0026#34;coverage\u0026#34;: { \u0026#34;lines\u0026#34;: 85.5, \u0026#34;branches\u0026#34;: 78.3, \u0026#34;functions\u0026#34;: 90.0 } } } Visualization Tools AWS Console Views\nTest result trends Coverage graphs Failure analysis Custom Reporting\n# Generate custom HTML report dotnet reportgenerator \\ -reports:\u0026#34;**/coverage.cobertura.xml\u0026#34; \\ -targetdir:\u0026#34;custom-report\u0026#34; \\ -reporttypes:Html_Dark Best Practices Regular Monitoring\nCheck results after each build Track trends over time Set up notifications Result Analysis\nInvestigate failures promptly Monitor coverage trends Document findings Report Management\nArchive historical reports Maintain documentation Share insights with team Troubleshooting Guide Missing Results\nCheck build configuration Verify report generation Review storage settings Incorrect Metrics\nValidate test configuration Check coverage settings Review exclusion filters Access Issues\nVerify IAM permissions Check S3 bucket policies Review security groups Integration with Development Tools IDE Integration { \u0026#34;dotnet-test-explorer.testProjectPath\u0026#34;: \u0026#34;**/*Tests.csproj\u0026#34;, \u0026#34;dotnet-test-explorer.autoWatch\u0026#34;: true } GitHub Integration name: Update PR with Test Results on: workflow_run: workflows: [\u0026#34;CodeBuild Tests\u0026#34;] types: [completed] Next Steps After understanding how to view and analyze test results, proceed to Fix Failures to learn how to address and resolve test failures effectively.\n"
},
{
	"uri": "//localhost:1313/3-automated-unit-test/3.5-fix-failures/",
	"title": "Fix Failures",
	"tags": [],
	"description": "",
	"content": "Fixing Test Failures Overview In this section, we\u0026rsquo;ll learn how to effectively diagnose, fix, and prevent test failures in our automated testing pipeline. We\u0026rsquo;ll cover debugging strategies, common failure patterns, and best practices for maintaining a robust test suite.\nImplementation Steps 1. Analyze Test Failures Review failure details: # Get detailed test results aws codebuild batch-get-test-reports \\ --report-arns \u0026lt;report-arn\u0026gt; \\ --query \u0026#39;testReports[].testCases[?status==`FAILED`]\u0026#39; Examine test logs: # Get build logs aws codebuild get-build-logs --id \u0026lt;build-id\u0026gt; 2. Common Failure Types and Solutions Assertion Failures // Before: Fragile Test [Fact] public void Add_ReturnsSum() { var result = _calculator.Add(2, 3); Assert.Equal(5, result); } // After: Robust Test [Theory] [InlineData(2, 3, 5)] [InlineData(-1, 1, 0)] [InlineData(0, 0, 0)] public void Add_ReturnsExpectedSum(int a, int b, int expected) { var result = _calculator.Add(a, b); Assert.Equal(expected, result); } Timing Issues // Before: Time-sensitive Test [Fact] public async Task Operation_CompletesInTime() { var result = await _service.LongOperation(); Assert.True(result.IsComplete); } // After: Reliable Test [Fact] public async Task Operation_CompletesInTime() { var cts = new CancellationTokenSource(TimeSpan.FromSeconds(5)); var result = await _service.LongOperation() .WithTimeout(cts.Token); Assert.True(result.IsComplete); } Environment Dependencies // Before: Environment-dependent Test [Fact] public void Config_LoadsCorrectly() { var config = Configuration.Load(); Assert.NotNull(config); } // After: Environment-independent Test [Fact] public void Config_LoadsCorrectly() { var mockConfig = new MockConfiguration(); var config = Configuration.Load(mockConfig); Assert.NotNull(config); } 3. Implement Test Fixes Create fix branch: git checkout -b fix/test-failures Apply fixes: # Update test files git add . git commit -m \u0026#34;fix: resolve test failures in calculator tests\u0026#34; Verify locally: dotnet test --filter \u0026#34;FailedTestName\u0026#34; Debugging Strategies Local Debugging { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Debug Tests\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;coreclr\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;dotnet\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;test\u0026#34;, \u0026#34;--filter\u0026#34;, \u0026#34;FailedTest\u0026#34;] } } Remote Debugging phases: build: commands: - dotnet test --collect:\u0026#34;Code Coverage\u0026#34; --logger \u0026#34;console;verbosity=detailed\u0026#34; Best Practices Test Isolation\nIndependent test cases Clean test environment Controlled dependencies Error Handling\nClear error messages Proper exception handling Meaningful assertions Test Maintenance\nRegular updates Remove flaky tests Document known issues Prevention Strategies Code Reviews # .github/pull_request_template.md ## Test Changes - [ ] Tests added/updated - [ ] All tests passing locally - [ ] No flaky tests introduced Quality Gates # buildspec.yml phases: post_build: commands: - | if [ \u0026#34;$TEST_FAILURES\u0026#34; -gt 0 ]; then echo \u0026#34;Quality gate failed: $TEST_FAILURES test failures\u0026#34; exit 1 fi Monitoring and Alerts aws cloudwatch put-metric-alarm \\ --alarm-name TestFailureRate \\ --metric-name FailedTests \\ --threshold 5 Next Steps After fixing test failures and implementing preventive measures, proceed to Parallel Execution to learn how to optimize test execution time.\n"
},
{
	"uri": "//localhost:1313/5-performance-testing/",
	"title": "Performance Testing",
	"tags": [],
	"description": "",
	"content": "Performance Testing Overview This module focuses on implementing performance testing in your CI/CD pipeline using AWS CodeBuild. You\u0026rsquo;ll learn how to write, execute, and analyze performance tests to ensure your application meets performance requirements.\nWhat You\u0026rsquo;ll Learn Writing Performance Tests\nTest scenarios design Load test implementation Stress test creation Performance metrics CI/CD Integration\nPipeline configuration Test automation Resource management Environment setup Results Export\nData collection Metrics aggregation Report generation Trend analysis Performance Analysis\nMetrics interpretation Bottleneck identification Optimization strategies Recommendations Prerequisites Before starting this module, ensure you have:\nCompleted Module 4 (Parallel Execution) Understanding of performance concepts Familiarity with k6 testing tool Basic knowledge of metrics analysis Time Estimation Total Module Time: ~3 hours Individual Section Time: 45 minutes each Module Structure Write Performance Tests\nTest scenario creation Performance metrics setup Integrate into CI/CD\nPipeline integration Automation setup Export Results\nData collection Report generation Analyze Performance\nAnalysis techniques Optimization strategies Expected Outcomes By the end of this module, you will have:\nCreated comprehensive performance tests Integrated tests into CI/CD pipeline Implemented results collection Analyzed performance metrics Identified optimization opportunities Let\u0026rsquo;s begin with Writing Performance Tests!\n"
},
{
	"uri": "//localhost:1313/2-environment-setup/2.5-webhook-verify/",
	"title": "Webhook Verification",
	"tags": [],
	"description": "",
	"content": "Verifying Webhook Integration Overview In this section, we\u0026rsquo;ll verify that the webhook integration between GitHub and AWS CodeBuild is working correctly. This ensures that our automated testing pipeline triggers automatically when code changes are pushed.\nPrerequisites CodeBuild project set up from previous section GitHub repository with webhook configured Required IAM permissions Implementation Steps 1. Verify Webhook Configuration Check webhook settings in GitHub:\nGo to repository settings Navigate to Webhooks Verify AWS CodeBuild webhook exists Check webhook status (should show recent deliveries) # List webhooks using GitHub CLI gh api repos/{owner}/{repo}/hooks 2. Test Push Trigger Make a small change to your repository: echo \u0026#34;# Test Change\u0026#34; \u0026gt;\u0026gt; README.md git add README.md git commit -m \u0026#34;test: verify webhook trigger\u0026#34; git push origin main Monitor build status: # Get latest build information aws codebuild list-builds-for-project --project-name dotnet-test-automation # Get detailed build information aws codebuild batch-get-builds --ids \u0026lt;build-id\u0026gt; 3. Test Pull Request Trigger Create a new branch: git checkout -b feature/test-webhook echo \u0026#34;## Testing PR Trigger\u0026#34; \u0026gt;\u0026gt; README.md git add README.md git commit -m \u0026#34;test: verify PR webhook\u0026#34; git push origin feature/test-webhook Create Pull Request: gh pr create --title \u0026#34;test: webhook verification\u0026#34; --body \u0026#34;Testing CodeBuild webhook trigger\u0026#34; Monitor PR checks in GitHub interface 4. Verify Build Logs Check CloudWatch Logs: Navigate to CloudWatch console Find log group: /aws/codebuild/dotnet-test-automation Review latest log streams Check CodeBuild Console: View build history Check build phases Verify test reports Troubleshooting Guide Common Issues Webhook Not Triggering # Check webhook delivery status gh api repos/{owner}/{repo}/hooks/{hook_id}/deliveries # Verify webhook configuration aws codebuild batch-get-projects --names dotnet-test-automation Build Failures Check IAM permissions Verify buildspec.yml syntax Review environment variables GitHub Connection Issues # Test GitHub connection gh auth status # Verify repository access gh repo view Webhook Security Secure Webhook Settings Use HTTPS endpoints only Configure secret token Limit webhook events Monitor Webhook Activity # View recent webhook deliveries gh api repos/{owner}/{repo}/hooks/{hook_id}/deliveries \\ --jq \u0026#39;.[0:5]|.[]|{id, status, delivered_at}\u0026#39; Best Practices Regular Verification Test webhook triggers periodically Monitor build success rates Review webhook delivery logs Error Handling Set up CloudWatch alarms Configure notification for failures Implement retry mechanisms Documentation Document webhook configuration Maintain troubleshooting guide Record common issues and solutions Next Steps After verifying webhook integration:\nProceed to Automated Unit Testing Set up monitoring and alerts Document the process for team reference "
},
{
	"uri": "//localhost:1313/6-security-testing/",
	"title": "Security Testing with CodeQL ",
	"tags": [],
	"description": "",
	"content": "In this section, you\u0026rsquo;ll integrate CodeQL security analysis into your GitHub repository. CodeQL helps detect common vulnerabilities such as injection, data leakage, and insecure logic using static analysis.\nüéØ Objectives Enable CodeQL analysis on GitHub Review and understand security alerts Fix identified vulnerabilities (if any) Disable security scanning if unnecessary üß© Steps Step Description Path 6.1 Enable CodeQL on GitHub 6.1-enable-codeql 6.2 Review and analyze alerts 6.2-review-alerts 6.3 Fix security vulnerabilities 6.3-fix-vulnerabilities 6.4 Disable CodeQL if needed 6.4-disable-if-needed "
},
{
	"uri": "//localhost:1313/7-monitoring-cost/",
	"title": "Monitoring, Reporting &amp; Cost Optimization ",
	"tags": [],
	"description": "",
	"content": "Once automated testing is in place, it\u0026rsquo;s crucial to monitor build processes, view test results, and optimize operational cost. AWS provides tools like CloudWatch Logs, Cost Explorer, and fine-tuned build configurations to help minimize CI/CD expenses.\nüéØ Objectives View test logs using CloudWatch Logs Analyze build costs with CodeBuild metrics Optimize build configuration (time, compute type) Estimate resource usage and cost üß© Steps Step Description Path 7.1 Monitor logs via CloudWatch 7.1-cloudwatch-logs 7.2 Analyze build cost 7.2-analyze-cost 7.3 Optimize CodeBuild configuration 7.3-optimize-config 7.4 Estimate usage and spending 7.4-estimate-usage "
},
{
	"uri": "//localhost:1313/8-clean-up/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/.net-8/",
	"title": ".NET 8",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/automated-testing/",
	"title": "Automated Testing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/aws/",
	"title": "AWS",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/ci/cd/",
	"title": "CI/CD",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/devops/",
	"title": "DevOps",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
}]